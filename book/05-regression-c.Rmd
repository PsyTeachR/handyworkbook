# Slopes

The slope in a simple linear regression can be defined as the change in predicted values of Y (the dependent variable) for every one unit change in X (the predictor variable). In the simple linear regression model shown below, the slope is written as $b_1$ (pronounced beta-one) . 

$$Y = b_{0} + b_{1}X + error$$

To put the slope into a physical context, say, for example, when measuring headsize to predict IQ, you find a slope of $b_1$ = 1.5, that would mean that for every increase of 1 in headsize, you would predict an increase of 1.5 in IQ.  

```{r we, message=FALSE, warning=FALSE, echo=FALSE}
dat_o <- tibble(Participants = 1:10,
              HeadSize = c(50.8, 63.5, 45.7, 25.4, 29.2, 49.5, 38.1, 30.5, 35.6, 58.4),
              IQ = c(107, 121, 106, 72, 85, 105, 93, 88, 97, 123))

m_x <- mean(dat_o$HeadSize) %>% round2(2)
m_y <- mean(dat_o$IQ) %>% round2(2)
sd_x <- sd(dat_o$HeadSize) %>% round2(2)
sd_y <- sd(dat_o$IQ) %>% round2(2)
cov_xy <- cov(dat_o$HeadSize, dat_o$IQ) %>% round2(2)
r_xy <- cor(dat_o$HeadSize, dat_o$IQ) %>% round2(3)
slope <- (r_xy * (sd_y/sd_x)) %>% round2(3)
intercept <- (m_y - slope * m_x) %>% round2(2)
HSp <- 60.1
IQp <- (intercept + slope * HSp) %>% round2(2)
HSp2 <- 59.4
IQp2 <- (intercept + slope * HSp2) %>% round2(2)
R2 <- (r_xy^2) %>% round2(3)

mod <- lm(IQ ~ HeadSize, data = dat_o) %>% anova() %>% tidy()
mod_df1 <- mod %>% filter(term == "HeadSize") %>% pull(df)
mod_df2 <- mod %>% filter(term == "Residuals") %>% pull(df)
mod_F <- mod %>% filter(term == "HeadSize") %>% pull(statistic) %>% round2(2)
mod_p <- ifelse(mod %>% filter(term == "HeadSize") %>% pull(p.value) < .001, 
                "< .001", 
                paste0("= ", mod %>% filter(term == "HeadSize") %>% pull(p.value) %>% round2(3)))
```

**the slope:**

The formula for the slope is:

$$b_{1} = \frac{cov_{(x, y)}}{s^2_{x}}$$
which is stated as the covariance of x and y ($cov_{(x,y)}$) divided by the variance of x ($s^2_{x}$). If we translate that into outcome and predictors then, given our Head Size is the predictor ($x$) and the IQ is the outcome ($y$), that becomes:

$$b_{1} = \frac{cov_{(HeadSize, IQ)}}{s^2_{HeadSize}}$$

But this can also be written in terms of the correlation between the two variables as opposed to the covariance, and this would look like:

$$b_{1} = r_{(x,y)} \times \frac{s_{y}}{s_{x}}$$

Which is stated as the correlation between x and y ($r_{(x,y)}$) multiplied by the standard deviation of y ($s_{y}$) divided by the standard deviation of x ($s_{x}$). [Remember that $s$ is the standard deviation of a variable, and $s^2$ is the variance of the variable.]

And again we can translate that back into terms of HeadSize ($x$) and IQ ($y$) as:

$$b_{1} = r \times \frac{s_{IQ}}{s_{HeadSize}}$$

Now if we think back to the correlation chapter we actually do know all the values we need, or at least can calculate them. Here is what we know from the correlation chapter:

* $cov_{(x,y)}$ = `r cov_xy`
* $s_{HeadSize}$ = `r sd_x`
* $s_{IQ}$ = `r sd_y`
* $r_{(x,y)}$ = `r r_xy`

And from that we can calculate the variance of HeadSize ($s^2_{HeadSize}$) and IQ ($s^2_{IQ}$), based on $s^2 = s \times s$, meaning:

* $s^2_{HeadSize}$ = $`r sd_x` \times `r sd_x`$ = `r sd_x^2`
* $s^2_{IQ}$ = $`r sd_y` \times `r sd_y`$ = `r sd_y^2`

So we could really use either formula for the slope as we have all the information, but for now let's use:

$$b_{1} = r \times \frac{s_{IQ}}{s_{HeadSize}}$$

And if we start to feed in the values, remembering that the IQ is ($y$ - the outcome) and HeadSize is ($x$ - the predictor) we get:

$$b_{1} = `r r_xy` \times \frac{`r sd_y`}{`r sd_x`}$$

And then if we sort out the fraction first, that becomes:

$$b_{1} = `r r_xy` \times `r sd_y/sd_x`$$

Which leads down to:

$$b_{1} = `r r_xy * (sd_y/sd_x)`$$

Giving a slope of $b_{1}$ = `r slope`, to three decimal places, meaning that for a 1 unit change in $x$ we get a `r slope` unit change in $y$. Or in other words, for a 1 unit change in HeadSize we get a `r slope` unit change in IQ.

