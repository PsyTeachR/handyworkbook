# Spearman Correlation

**The Spearman correlation** otherwise known as the Spearman rank correlation coefficient measures the relationship between two variables when that relationship is monotonic. It can be used for relationships that are linear or non-linear. 

As in the Pearson Correlation, the Spearman correlation measures the covariance between two variables, or the shared variance, and then standardises that measure between the values of -1 and 1 with -1 being the perfect negative relationship and 1 being the perfect positive relationship.

The key difference between the Spearman Correlation and the Pearson Correlation is that the Spearman Correlation is based on the rank order of data - i.e. the raw data is converted to ordinal ranks.

If all of the ranks within each variable are unique - i.e. there are no `r glossary("tied ranks", def = "when two or more values/observations within a variable are identical and assume the same rank order value", display = "tied ranks", link = FALSE)` - then the formula for the Spearman Correlation is as follows:

$$\rho = 1 - \frac{6 \times \sum{d_i^2}}{n(n^2-1)}$$
Let's start with a rather simple dataset of 10 participants measured on two scales, A and B. Here is our data:

```{r, echo=FALSE}
dat <- tibble(id = 1:10, a = 11:20, b = 20:11)

dat2 <- dat %>% 
  mutate(a_rank = rank(a,
                       ties.method = "first"),
         b_rank = rank(b,
                       ties.method = "first"),
         d = a_rank - b_rank,
         `d^2` = d^2)

knitr::kable(dat)
```

And to orientate ourselves a little, the table shows Participant 1 scored 11 for scale A and 20 for scale B, and Participant 2 scored 12 for scale A and 19 for scale B.

**Ranking data**

The first step in this analysis is to convert the data in both scales into their respective rank order starting from the smallest value in each scale. For example, the smallest value would get the rank of 1, the second smallest value would get the rank of 2, and so on and so on. If we do that with our data in scale A, we get the following:

```{r, echo=FALSE}
dat2 %>% 
  select(id, a, a_rank, b) %>%
  knitr::kable()
```

And then do the same for the scores in scale b, again starting from the smallest value and giving it a rank of 1, and so on. This would look like:

```{r, echo=FALSE}
dat2 %>% 
  select(id, a, a_rank, b, b_rank) %>%
  knitr::kable()
```

Now we have both scales in their respective rank orders, we can now calculate $d$, each individual participants difference between their ranks on the two scales. For example, with Participant 1, they have a rank of `r dat2 %>% filter(id == 1) %>% pull(a_rank)` on scale A, and a rank of `r dat2 %>% filter(id == 1) %>% pull(b_rank)` on scale B. This would give them a $d$ of `r dat2 %>% filter(id == 1) %>% pull(a_rank)` - `r dat2 %>% filter(id == 1) %>% pull(b_rank)` = `r dat2 %>% filter(id == 1) %>% pull(a_rank) - dat2 %>% filter(id == 1) %>% pull(b_rank)`

If we do that for all the participants we then get:

```{r, echo=FALSE}
dat2 %>% 
  select(id, a, a, a_rank, b, b_rank, d) %>%
  knitr::kable()
```

And finally we want to square each of those individual values to give us a $d^2$ for each participant. For example, Participant 1, with a value of d =`r dat2 %>% filter(id == 1) %>% pull(a_rank) - dat2 %>% filter(id == 1) %>% pull(b_rank)`, would have a $d^2$ of `r dat2 %>% filter(id == 1) %>% pull(a_rank) - dat2 %>% filter(id == 1) %>% pull(b_rank)` $\times$ `r dat2 %>% filter(id == 1) %>% pull(a_rank) - dat2 %>% filter(id == 1) %>% pull(b_rank)` = `r (dat2 %>% filter(id == 1) %>% pull(a_rank) - dat2 %>% filter(id == 1) %>% pull(b_rank))^2` 

And if we do that for all the participants we then get:

```{r, echo=FALSE}
dat2 %>%
  knitr::kable()
```

Now that we have calculated $d^2$ for each participant we can sum them altogether to give us $\sum{d_i^2}$ as follows:

```{r, echo=FALSE}
p1 <- dat2 %>% filter(id == 1) %>% pull(`d^2`)
p2 <- dat2 %>% filter(id == 2) %>% pull(`d^2`)
p3 <- dat2 %>% filter(id == 3) %>% pull(`d^2`)
p4 <- dat2 %>% filter(id == 4) %>% pull(`d^2`)
p5 <- dat2 %>% filter(id == 5) %>% pull(`d^2`)
p6 <- dat2 %>% filter(id == 6) %>% pull(`d^2`)
p7 <- dat2 %>% filter(id == 7) %>% pull(`d^2`)
p8 <- dat2 %>% filter(id == 8) %>% pull(`d^2`)
p9 <- dat2 %>% filter(id == 9) %>% pull(`d^2`)
p10 <- dat2 %>% filter(id == 10) %>% pull(`d^2`)
sumd2 <- dat2 %>% pull(`d^2`) %>% sum()
n <- dim(dat2)[1]
```

$$\sum{d_i^2} = `r p1` + `r p2` + `r p3` + `r p4` + `r p5` + `r p6`+ `r p7` + `r p8` + `r p9` + `r p10`$$

Which gives us:

$$\sum{d_i^2} = `r p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10`$$

**The Correlation**

We can now return to our formula:

$$\rho = 1 - \frac{6 \times \sum{d_i^2}}{n(n^2-1)}$$

And we know that:

* $\sum{d_i^2} = `r sumd2`$
* and $n = `r n`$

Which we can then put into our formula giving:

$$\rho = 1 - \frac{6 \times `r sumd2`}{`r n`(`r n`^2-1)}$$

If we resolve the square in the bottom half of the formula, that gives:

$$\rho = 1 - \frac{6 \times `r sumd2`}{`r n`(`r n^2`-1)}$$

And then subtract the 1, giving:

$$\rho = 1 - \frac{6 \times `r sumd2`}{`r n`(`r n^2-1`)}$$
Now the bottom half can really be read as:

$$\rho = 1 - \frac{6 \times `r sumd2`}{`r n` \times (`r n^2-1`)}$$

Which, when we do the multiplication on the bottom half, gives us:

$$\rho = 1 - \frac{6 \times `r sumd2`}{`r n * (n^2-1)`}$$

And if we then do the multiplication on the top half, we get:

$$\rho = 1 - \frac{`r 6 * sumd2`}{`r n * (n^2-1)`}$$

We then divide the top half by the bottom half to give us:

$$\rho = 1 - `r (6 * sumd2)/(n * (n^2-1))`$$

And we do the final subtraction, leaving us:

$$\rho = `r 1 - ((6 * sumd2)/(n * (n^2-1)))`$$

```{r cor_vals, echo = FALSE}
corval <- cor.test(dat$a, dat$b, method = "spearman") %>% pluck("estimate") %>% round2(3)
df <- cor.test(dat$a, dat$b, method = "pearson") %>% pluck("parameter")
realt <- cor.test(dat$a, dat$b, method = "pearson") %>% pluck("statistic") %>% abs()
realp <- pt(realt, df, lower.tail = FALSE)*2

strongcorp <- .45
medcorp <- .25
weakcorp <- .08
weakcorn <- -.08
medcorn <- -.25
strongcorn <- -.45


rcrit <- sqrt(qt(1-.975, df, lower.tail = FALSE)^2/(qt(1-.975, df, lower.tail = FALSE)^2 + df)) %>% round2(3)

verbdesc <- case_when(corval >= strongcorp ~ "a strong positive",
          medcorp <= corval & corval < strongcorp ~ "a medium positive",
          weakcorp <= corval & corval < medcorp ~ "a weak positive",
          corval > medcorn & corval <= weakcorn ~ "a weak negative",
          corval > strongcorn & corval <= medcorn ~ "a medium negative",
          corval <= strongcorn ~ "a strong negative",
          TRUE ~ "no")


```


Meaning that we have a Spearman rho of $\rho = `r 1 - ((6 * sumd2)/(n * (n^2-1)))`$.  Following Cohen's (1988) proposed cut-offs shown below, would be considered to show `r verbdesc` relationship. Remember that these cut-offs are just guides so a value just below a cut-off, e.g. r = .48, would probably be described as, for example, a strong positive relationship.

|Relationship Description (Cohen, 1988)| r-value |
|---:|:---:|
|Strong positive relationship| r = .5|
|Medium positive relationship| r = .3|
|Weak positive relationship| r = .1|
|No relationship| r = 0| 
|Weak negative relationship| r = -.1|
|Medium negative relationship| r = -.3|
|Strong negative relationship| r = -.5|

**Degrees of Freedom**

Great! Now, finally, to determine significance we first need the degrees of freedom for the test. The degrees of freedom formula is:

$$df = N - 2$$

And we already know that $N= `r dim(dat)[1]`$ and putting them into the equation we get:

$$df = `r dim(dat)[1]` - 2$$

Which reduces to:

$$df = `r dim(dat)[1] - 2`$$


Meaning that we find a $df = `r dim(dat)[1] - 2`$

**Writing up**

The standard APA format for writing up a Spearman correlation is usually:

<p align = "center">**$\rho$(df) = r-value, p = p-value**</p>

or alternatively

<p align = "center">**$r_s$(df) = r-value, p = p-value**</p>

From above we know the df and we know the r-value and what we need now is the significance.

**Determining Significance**

If we were to look at a critical values look-up table for $df = `r df`$ and $\alpha = .05$, we would see that the critical value is $r_{crit} = `r rcrit`$. Given that our r-value, ignoring polarity and just looking at the absolute value, so $r = `r corval`$, is `r ifelse(abs(corval) >= rcrit, "equal to or larger than", "smaller than")` our $r_{crit}$ then we can say our result is `r ifelse(abs(corval) >= rcrit, "significant", "not significant")`, and as such would be written up as $r_s$(`r df`) = `r corval`, `r ifelse(abs(corval) >= rcrit, "p < .05", "p > .05")`. 

**Remember:** If you were writing this up as a report, and analysed the data in R, then you would see the p-value was actually p = `r format(realp, scientific = FALSE)`, and would be written up as `r ifelse(realp < .001, "p < .001", paste("p = ", realp))`

## Look-Up table

Remembering that the $r_{crit}$ value is the smallest r-value you need to find a significant effect, find the $r_{crit}$ value for your df, assuming $\alpha = .05$. If the $r$-value you calculated is equal to or larger than $r_{crit}$ value then your test is significant.

|df | $\alpha = .05$|
|:-:|:-------------:|
|1  |`r sqrt(qt(1-.975, 1, lower.tail = FALSE)^2/(qt(1-.975, 1, lower.tail = FALSE)^2 + 1)) %>% round2(3)`|
|2  |`r sqrt(qt(1-.975, 2, lower.tail = FALSE)^2/(qt(1-.975, 2, lower.tail = FALSE)^2 + 2)) %>% round2(3)`|
|3  |`r sqrt(qt(1-.975, 3, lower.tail = FALSE)^2/(qt(1-.975, 3, lower.tail = FALSE)^2 + 3)) %>% round2(3)`|
|4  |`r sqrt(qt(1-.975, 4, lower.tail = FALSE)^2/(qt(1-.975, 4, lower.tail = FALSE)^2 + 4)) %>% round2(3)`|
|5  |`r sqrt(qt(1-.975, 5, lower.tail = FALSE)^2/(qt(1-.975, 5, lower.tail = FALSE)^2 + 5)) %>% round2(3)`|
|6  |`r sqrt(qt(1-.975, 6, lower.tail = FALSE)^2/(qt(1-.975, 6, lower.tail = FALSE)^2 + 6)) %>% round2(3)`|
|7  |`r sqrt(qt(1-.975, 7, lower.tail = FALSE)^2/(qt(1-.975, 7, lower.tail = FALSE)^2 + 7)) %>% round2(3)`|
|8  |`r sqrt(qt(1-.975, 8, lower.tail = FALSE)^2/(qt(1-.975, 8, lower.tail = FALSE)^2 + 8)) %>% round2(3)`|
|9  |`r sqrt(qt(1-.975, 9, lower.tail = FALSE)^2/(qt(1-.975, 9, lower.tail = FALSE)^2 + 9)) %>% round2(3)`|
|10 |`r sqrt(qt(1-.975, 10, lower.tail = FALSE)^2/(qt(1-.975, 10, lower.tail = FALSE)^2 + 10)) %>% round2(3)`|
|15 |`r sqrt(qt(1-.975, 15, lower.tail = FALSE)^2/(qt(1-.975, 15, lower.tail = FALSE)^2 + 15)) %>% round2(3)`|
|20 |`r sqrt(qt(1-.975, 20, lower.tail = FALSE)^2/(qt(1-.975, 20, lower.tail = FALSE)^2 + 20)) %>% round2(3)`|
|30 |`r sqrt(qt(1-.975, 30, lower.tail = FALSE)^2/(qt(1-.975, 30, lower.tail = FALSE)^2 + 30)) %>% round2(3)`|
|40 |`r sqrt(qt(1-.975, 40, lower.tail = FALSE)^2/(qt(1-.975, 40, lower.tail = FALSE)^2 + 40)) %>% round2(3)`|
|50 |`r sqrt(qt(1-.975, 50, lower.tail = FALSE)^2/(qt(1-.975, 50, lower.tail = FALSE)^2 + 50)) %>% round2(3)`|
|60 |`r sqrt(qt(1-.975, 60, lower.tail = FALSE)^2/(qt(1-.975, 60, lower.tail = FALSE)^2 + 60)) %>% round2(3)`|
|70 |`r sqrt(qt(1-.975, 70, lower.tail = FALSE)^2/(qt(1-.975, 70, lower.tail = FALSE)^2 + 70)) %>% round2(3)`|
|80 |`r sqrt(qt(1-.975, 80, lower.tail = FALSE)^2/(qt(1-.975, 80, lower.tail = FALSE)^2 + 80)) %>% round2(3)`|
|90 |`r sqrt(qt(1-.975, 90, lower.tail = FALSE)^2/(qt(1-.975, 90, lower.tail = FALSE)^2 + 90)) %>% round2(3)`|
|100|`r sqrt(qt(1-.975, 100, lower.tail = FALSE)^2/(qt(1-.975, 100, lower.tail = FALSE)^2 + 100)) %>% round2(3)`|
