[
["index.html", "A Handy Workbook for Research Methods and Statistics Overview", " A Handy Workbook for Research Methods and Statistics Last Update: 2021-05-25 Overview PLEASE NOTE THIS BOOK IS IN EARLY STAGES OF DEVELOPMENT AND WILL BE ADDED TO AND MAY VERY WELL CURRENTLY CONTAIN MISTAKES Authors: Phil McAleer Aim: A Handy Workbook to help students understand Research Methods and Statistics through worked examples and self-tests. Contact: This book is a living document and will be regularly checked and updated for improvements. Should you have any issues using the book or queries, please contact Phil McAleer. R Version: This book has been written with R version 3.5.1 (2018-07-02) Randomising Seed: In chapters that use some level of randomisation, where we have remembered, the seed is set as 1409. "],
["foreword.html", "Foreword", " Foreword This book is designed to help people understand the statistical tests commonly taught in a Psychology undergraduate course by walking through the tests in a step-by-step process. When we run analytical tests we often employ a statistical software to carry out the tests for us and our role, as researchers, is reduced to inputting the data in a corret format and interpreting the output. However, for us to understand and verify that the software is working correctly we must have some working knowledge of the test being used. For example, if a t-test comes back positive or negative, by understanding the processes involved in the calculation we can understand why that happens and what it means in terms of the research question we have set. Likewise, by understanding how degrees of freedom relate to different tests we can verify the output from the software, making sure that the values we get are appropriate. So whilst we, as researchers, will rarely have to run a full analysis by hand, and from memory, to fully understand the analyses that are the bases of the papers we write, it is important to know our way around those tests and how they work; it is unwise to just use an analytical software blindly without having some grasp of what it is doing and then using that output to make claims about human behavior. We hope that this book helps in that understanding and in turn improves your research practice. If you have any suggestions for improvements to this book or would like to see a test added, please do let us know! "],
["descriptives.html", "Chapter 1 Descriptives 1.1 Worked Examples", " Chapter 1 Descriptives 1.1 Worked Examples The Data To look at the mean, variance, standard deviation, standard error, 95% Confidence Intervals, and z-scores, we will use the following data from 10 participants: Participants X 1 8 2 11 3 6 4 3 5 1 6 6 7 1 8 0 9 1 10 4 1.1.1 The Mean The symbol for the mean is generally written as \\(\\overline{X}\\) (pronounced as X-bar) and the formula for the mean is: \\[\\overline{X} = \\frac{\\sum_i^n{x_i}}{n}\\] Which reads as sum (\\(\\sum\\)) all the values from \\(i\\) (the first value) to \\(n\\) (the last value) and then divide that number by the total number of values (\\(n\\)). If we start to fill in the information from above we get: \\[\\overline{X} = \\frac{8 + 11 + 6 +3 +1 +6 +1 +0 +1 +4}{10}\\] And if we sum all the values on the top half together we get: \\[\\overline{X} = \\frac{41}{10}\\] And finally divide the top half by the bottom half, leaving: \\[\\overline{X} = 4.1\\] We find that the mean, rounded to two decimal places, is \\(\\overline{X} = 4.1\\) 1.1.2 Variance The symbol for the variance is generally written as \\(s^2\\) (pronounced as sigma-squared) but can also be written as \\(Var\\). The formula for the variance is: \\[s^2 = \\frac{\\sum_i^n(x_{i} - \\overline{x})^2}{n-1}\\] So let’s again start by filling in the numbers to the formula: \\[s^2 = \\frac{(8 - 4.1)^2 + (11 - 4.1)^2 + (6 - 4.1)^2 + (3 - 4.1)^2 + (1 - 4.1)^2 + \\\\(6 - 4.1)^2 + (1 - 4.1)^2 + (0 - 4.1)^2 + (1 - 4.1)^2 + (4 - 4.1)^2}{10 - 1}\\] And we do the subtractions in all those brackets of the top half and the one on the bottom: \\[s^2 = \\frac{(3.9)^2 + (6.9)^2 + (1.9)^2 + (-1.1)^2 + (-3.1)^2 + \\\\(1.9)^2 + (-3.1)^2 + (-4.1)^2 + (-3.1)^2 + (-0.1)^2}{9}\\] Then we square all those values on the top half: \\[s^2 = \\frac{15.21 + 47.61+ 3.61+ 1.21+ 9.61+ 3.61+ 9.61+ 16.81+ 9.61+ 0.01}{9}\\] Sum those values together: \\[s^2 = \\frac{116.9}{9}\\] And divide the top half by the bottom half: \\[s^2 = 12.9888889\\] Showing that the variance, rounded to two decimal places is, \\(s^2 = 12.99\\) 1.1.3 Standard Deviation The symbol for the standard deviation is generally written as \\(s\\) (pronounced as sigma) but can also be written as \\(SD\\). The formula for the standard deviation is: \\[s = \\sqrt\\frac{\\sum_i^n(x_{i} - \\overline{x})^2}{n-1}\\] So let’s again start by filling in the numbers to the formula: \\[s = \\sqrt\\frac{(8 - 4.1)^2 + (11 - 4.1)^2 + (6 - 4.1)^2 + (3 - 4.1)^2 + (1 - 4.1)^2 + \\\\(6 - 4.1)^2 + (1 - 4.1)^2 + (0 - 4.1)^2 + (1 - 4.1)^2 + (4 - 4.1)^2}{10 - 1}\\] And we do the subtractions in all those brackets of the top half and the one on the bottom: \\[s = \\sqrt\\frac{(3.9)^2 + (6.9)^2 + (1.9)^2 + (-1.1)^2 + (-3.1)^2 + \\\\(1.9)^2 + (-3.1)^2 + (-4.1)^2 + (-3.1)^2 + (-0.1)^2}{9}\\] Then we square all those values on the top half: \\[s = \\sqrt\\frac{15.21 + 47.61+ 3.61+ 1.21+ 9.61+ 3.61+ 9.61+ 16.81+ 9.61+ 0.01}{9}\\] Sum those values together: \\[s = \\sqrt\\frac{116.9}{9}\\] Divide the top half by the bottom half: \\[s = \\sqrt{12.9888889}\\] And finally take the square root: \\[s = 3.6040101\\] We find that the standard deviation, rounded to two decimal places, is \\(s = 3.6\\) 1.1.4 Var to SD Remembering that the standard deviation is the square root of the variance, then if you know the variance (\\(s^2\\)) and you need the standard deviation (\\(s\\)) then you can do: \\[s = \\sqrt{s^2}\\] In our example, \\(s^2 = 12.9888889\\) and so \\[s = \\sqrt{12.9888889}\\] giving: \\[s = 3.6040101\\] 1.1.5 SD to Var Sometimes you need to go from the standard deviation to the variance, and remembering that the variance is the standard deviation squared, or the standard deviation multiplied by itself, then you can do: \\[s^2 = s \\times s\\] And if we know that the standard deviation in our example is \\(s = 3.6040101\\) then: \\[s^2 = 3.6040101 \\times 3.6040101\\] giving us: \\[s^2 = 12.9888889\\] 1.1.6 Z-Scores Any value on a continuous scale can be converted to a z-score (standard deviation units) through the formula: \\[z = \\frac{x - \\overline{x}}{s_{x}}\\] which can be read as the value (\\(x\\)) minus the mean value of the data (\\(\\overline{x}\\)), divided by the standard deviation of the data (\\(s_{X}\\)). For example, if we look at Participant 1: \\(x = 8\\) \\(\\overline{x} = 4.1\\) \\(s_{x} = 3.6040101\\) Which if we fill in the formula we get: \\[z = \\frac{8 - 4.1}{3.6040101}\\] And if we sort our the top half first: \\[z = \\frac{3.9}{3.6040101}\\] Leaving us with: \\[z = 1.0821279\\] So Participant 1, when converted to a z-score and rounded to two decimal places, would have z = 1.08 1.1.7 Standard Error of the Mean The symbol for the standard error is usually \\(SE\\) but can also be written as \\(SEM\\) when specificaly about the mean - short for standard error of the mean. The formula for the standard error is the standard deviation (\\(s\\)) divided by the square root of the number of observations (\\(\\sqrt{n}\\)), and is written as: \\[SE = \\frac{s}{\\sqrt{n}}\\] but can also be written as: \\[SE = \\frac{SD}{\\sqrt{n}}\\] We know that our \\(SD = 3.6040101\\) and that we have \\(n = 10\\) participants, so: \\[SE = \\frac{3.6040101}{\\sqrt{10}}\\] And if we do the square root of the bottom half (denominator): \\[SE = \\frac{3.6040101}{3.1622777}\\] And divide the top by the bottom: \\[SE = 1.1396881\\] We find that the standard error, rounded to two decimal places, is \\(SE = 1.14\\) 1.1.8 Confidence Intervals We will specifically focus on the 95% Confidence Interval using the cut-off value (assuming \\(\\alpha = .05\\) and two-tailed) of \\(z = 1.96\\). The key formulas are: \\[Upper \\space 95\\%CI = \\overline{x} + (z \\times SE)\\] And \\[Lower \\space 95\\%CI = \\overline{X} - (z \\times SE)\\] We know that: \\(\\overline{x} = 4.1\\) \\(SE = 1.1396881\\) \\(z = 1.96\\) Upper 95% CI Dealing with the Upper 95% CI we get: \\[Upper \\space 95\\%CI = 4.1 + (1.96 \\times 1.1396881)\\] And if we sort out the bracket first: \\[Upper \\space 95\\%CI = 4.1 + 2.2337886\\] Which leaves us with: \\[Upper \\space 95\\%CI = 6.3337886\\] Lower 95% CI And now the Lower 95% CI we get: \\[Lower \\space 95\\%CI = 4.1 - (1.96 \\times 1.1396881)\\] And if we sort out the bracket first: \\[Lower \\space 95\\%CI = 4.1 - 2.2337886\\] Which leaves us with: \\[Lower \\space 95\\%CI = 1.8662114\\] And if we round both those values to two decimal places, then you get \\(Lower \\space 95\\%CI = 1.87\\) and \\(Upper \\space 95\\%CI = 6.33\\) 1.1.9 The Write-Up And if we were to do a decent presentation of this data, including a measure of central tendency (the mean), a measure of spread relating to the sample (the standard deviation), and a measure of spread relating to the population (the 95% CI), then it would start as something like: “We ran 10 participants (M = 4.1, SD = 3.6, 95%CI = [1.87, 6.33])…..” "],
["one-sample-chi-square.html", "Chapter 2 One-Sample Chi-Square 2.1 The Worked Example 2.2 Test Yourself 2.3 ChiSquare Look-up Table", " Chapter 2 One-Sample Chi-Square 2.1 The Worked Example Here is our data: Values A B C D Observed 4 5 8 15 And if we add on a column showing the total number of participants, adding all the numbers in the different conditions together, (i.e. 4 + 5 + 8 + 15 = 32), then we get: Values A B C D Total Observed 4 5 8 15 32 Now the formula for the chi-square is: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] The Expected values for each condition, in a one-sample chi-square assuming a uniform (equal) distribution is calculated by \\(N \\times \\frac{1}{k}\\) where \\(k\\) is the number of conditions and \\(N\\) is the total number of participants. This can also be written more straightforward as \\(N/k\\). That means that in our example the expected value in each condition would be: \\[Expected = \\frac{N}{k} = \\frac{32}{4} = 8\\] Let’s now add those Expected values to our table which looks like: Values A B C D Total Observed 4 5 8 15 32 Expected 8 8 8 8 32 We now have our data, let’s start putting it into the formula, which we said was: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] Which really means: \\[\\chi^2 = \\frac{(Observed_{A} - Expected_{A})^2}{Expected_{A}} + \\frac{(Observed_{B} - Expected_{B})^2}{Expected_{B}} + \\frac{(Observed_{C} - Expected_{C})^2}{Expected_{C}} + \\\\ \\frac{(Observed_{D} - Expected_{D})^2}{Expected_{D}}\\] So \\[\\chi^2 = \\frac{(4 - 8)^2}{8}+\\frac{(5 - 8)^2}{8}+\\frac{(8 - 8)^2}{8}+\\frac{(15 - 8)^2}{8}\\] Which becomes: \\[\\chi^2 = \\frac{(-4)^2}{8} + \\frac{(-3)^2}{8} + \\frac{(0)^2}{8} + \\frac{(7)^2}{8}\\] And if we now square the top halves (the numerators): \\[\\chi^2 = \\frac{16}{8} + \\frac{9}{8} + \\frac{0}{8} + \\frac{49}{8}\\] Then divide the top half by the bottom half for each condition: \\[\\chi^2 = {2}+{1.125}+{0}+{6.125}\\] And finally add them altogether \\[\\chi^2 = 9.25\\] So we find that \\(\\chi^2 = 9.25\\) The degrees of freedom in this test is \\(k - 1\\) and given that we have 4 conditions: \\[df = k - 1\\] \\[df = 4 - 1\\] \\[df = 3\\] The effect size A common effect size for the one-sample chi-square test is \\(\\phi\\) (pronounced “ph-aye” and can be written as “phi”). The formula for \\(\\phi\\) is: \\[\\phi = \\sqrt\\frac{\\chi^2}{N}\\] And if we know that \\(\\chi^2 =9.25\\) and that \\(N = 32\\), then putting them into the formula we get: \\[\\phi = \\sqrt\\frac{9.25}{32}\\] \\[\\phi = 0.5376453\\] The write-up If we were to look at a critical value look-up table, we would see that the critical value associated with a \\(df = 3\\) at \\(\\alpha = .05\\), to three decimal places, is \\(\\chi^2_{crit} = 7.815\\). As the chi-square value of this test (i.e. \\(\\chi^2 = 9.25\\)) is larger than \\(\\chi^2_{crit}\\) then we can say that our test is significant, and as such would be written up as \\(\\chi^2(df = 3, N = 32) = 9.25,p &lt; .05\\). Finally, if our test was significant then all we need to do is state the condition with the highest frequency (i.e. the mode), which in this case is Condition D 2.2 Test Yourself 2.2.1 DataSet 1 Here is our data: Values A B C D Observed 14 19 2 20 Show me the working and answer And if we add on a column showing the total number of participants, adding all the numbers in the different conditions together, (i.e. 14 + 19 + 2 + 20 = 55), then we get: Values A B C D Total Observed 14 19 2 20 55 Now the formula for the chi-square is: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] The Expected values for each condition, in a one-sample chi-square assuming a uniform (equal) distribution is calculated by \\(N \\times \\frac{1}{k}\\) where \\(k\\) is the number of conditions and \\(N\\) is the total number of participants. This can also be written more straightforward as \\(N/k\\). That means that in our example the expected value in each condition would be: \\[Expected = \\frac{N}{k} = \\frac{55}{4} = 13.75\\] Let’s now add those Expected values to our table which looks like: Values A B C D Total Observed 14.00 19.00 2.00 20.00 55 Expected 13.75 13.75 13.75 13.75 55 We now have our data, let’s start putting it into the formula, which we said was: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] So \\[\\chi^2 = \\frac{(14 - 13.75)^2}{13.75}+\\frac{(19 - 13.75)^2}{13.75}+\\frac{(2 - 13.75)^2}{13.75}+\\frac{(20 - 13.75)^2}{13.75}\\] Which becomes: \\[\\chi^2 = \\frac{(0.25)^2}{13.75} + \\frac{(5.25)^2}{13.75} + \\frac{(-11.75)^2}{13.75} + \\frac{(6.25)^2}{13.75}\\] And if we now square the top halves (the numerators): \\[\\chi^2 = \\frac{0.0625}{13.75} + \\frac{27.5625}{13.75} + \\frac{138.0625}{13.75} + \\frac{39.0625}{13.75}\\] Then divide the top half by the bottom half for each condition: \\[\\chi^2 = {0.0045455}+{2.0045455}+{10.0409091}+{2.8409091}\\] And finally add them altogether \\[\\chi^2 = 14.8909091\\] So we find that \\(\\chi^2 = 14.8909091\\) The degrees of freedom in this test is \\(k - 1\\) and given that we have 4 conditions: \\[df = k - 1\\] \\[df = 4 - 1\\] \\[df = 3\\] The effect size A common effect size for the one-sample chi-square test is \\(\\phi\\) (pronounced “ph-aye” and can be written as “phi”). The formula for \\(\\phi\\) is: \\[\\phi = \\sqrt\\frac{\\chi^2}{N}\\] And if we know that \\(\\chi^2 =14.8909091\\) and that \\(N = 55\\), then putting them into the formula we get: \\[\\phi = \\sqrt\\frac{14.8909091}{55}\\] \\[\\phi = 0.5203305\\] The write-up If we were to look at a critical value look-up table, we would see that the critical value associated with a \\(df = 3\\) at \\(\\alpha = .05\\), to three decimal places, is \\(\\chi^2_{crit} = 7.815\\). As the chi-square value of this test (i.e. \\(\\chi^2 = 14.8909091\\)) is larger than \\(\\chi^2_{crit}\\) then we can say that our test is significant, and as such would be written up as \\(\\chi^2(df = 3, N = 55) = 14.8909091,p &lt; .05\\). Finally, if our test was significant then all we need to do is state the condition with the highest frequency (i.e. the mode), which in this case is Condition D 2.2.2 DataSet 2 Here is our data: Values A B C D Observed 2 8 7 20 Show me the working and answer And if we add on a column showing the total number of participants, adding all the numbers in the different conditions together, (i.e. 2 + 8 + 7 + 20 = 37), then we get: Values A B C D Total Observed 2 8 7 20 37 Now the formula for the chi-square is: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] The Expected values for each condition, in a one-sample chi-square assuming a uniform (equal) distribution is calculated by \\(N \\times \\frac{1}{k}\\) where \\(k\\) is the number of conditions and \\(N\\) is the total number of participants. This can also be written more straightforward as \\(N/k\\). That means that in our example the expected value in each condition would be: \\[Expected = \\frac{N}{k} = \\frac{37}{4} = 9.25\\] Let’s now add those Expected values to our table which looks like: Values A B C D Total Observed 2.00 8.00 7.00 20.00 37 Expected 9.25 9.25 9.25 9.25 37 We now have our data, let’s start putting it into the formula, which we said was: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] So \\[\\chi^2 = \\frac{(2 - 9.25)^2}{9.25}+\\frac{(8 - 9.25)^2}{9.25}+\\frac{(7 - 9.25)^2}{9.25}+\\frac{(20 - 9.25)^2}{9.25}\\] Which becomes: \\[\\chi^2 = \\frac{(-7.25)^2}{9.25} + \\frac{(-1.25)^2}{9.25} + \\frac{(-2.25)^2}{9.25} + \\frac{(10.75)^2}{9.25}\\] And if we now square the top halves (the numerators): \\[\\chi^2 = \\frac{52.5625}{9.25} + \\frac{1.5625}{9.25} + \\frac{5.0625}{9.25} + \\frac{115.5625}{9.25}\\] Then divide the top half by the bottom half for each condition: \\[\\chi^2 = {5.6824324}+{0.1689189}+{0.5472973}+{12.4932432}\\] And finally add them altogether \\[\\chi^2 = 18.8918919\\] So we find that \\(\\chi^2 = 18.8918919\\) The degrees of freedom in this test is \\(k - 1\\) and given that we have 4 conditions: \\[df = k - 1\\] \\[df = 4 - 1\\] \\[df = 3\\] The effect size A common effect size for the one-sample chi-square test is \\(\\phi\\) (pronounced “ph-aye” and can be written as “phi”). The formula for \\(\\phi\\) is: \\[\\phi = \\sqrt\\frac{\\chi^2}{N}\\] And if we know that \\(\\chi^2 =18.8918919\\) and that \\(N = 37\\), then putting them into the formula we get: \\[\\phi = \\sqrt\\frac{18.8918919}{37}\\] \\[\\phi = 0.714557\\] The write-up If we were to look at a critical value look-up table, we would see that the critical value associated with a \\(df = 3\\) at \\(\\alpha = .05\\), to three decimal places, is \\(\\chi^2_{crit} = 7.815\\). As the chi-square value of this test (i.e. \\(\\chi^2 = 18.8918919\\)) is larger than \\(\\chi^2_{crit}\\) then we can say that our test is significant, and as such would be written up as \\(\\chi^2(df = 3, N = 37) = 18.8918919,p &lt; .05\\). Finally, if our test was significant then all we need to do is state the condition with the highest frequency (i.e. the mode), which in this case is Condition D 2.2.3 DataSet 3 Here is our data: Values A B C D Observed 5 10 14 11 Show me the working and answer And if we add on a column showing the total number of participants, adding all the numbers in the different conditions together, (i.e. 5 + 10 + 14 + 11 = 40), then we get: Values A B C D Total Observed 5 10 14 11 40 Now the formula for the chi-square is: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] The Expected values for each condition, in a one-sample chi-square assuming a uniform (equal) distribution is calculated by \\(N \\times \\frac{1}{k}\\) where \\(k\\) is the number of conditions and \\(N\\) is the total number of participants. This can also be written more straightforward as \\(N/k\\). That means that in our example the expected value in each condition would be: \\[Expected = \\frac{N}{k} = \\frac{40}{4} = 10\\] Let’s now add those Expected values to our table which looks like: Values A B C D Total Observed 5 10 14 11 40 Expected 10 10 10 10 40 We now have our data, let’s start putting it into the formula, which we said was: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] So \\[\\chi^2 = \\frac{(5 - 10)^2}{10}+\\frac{(10 - 10)^2}{10}+\\frac{(14 - 10)^2}{10}+\\frac{(11 - 10)^2}{10}\\] Which becomes: \\[\\chi^2 = \\frac{(-5)^2}{10} + \\frac{(0)^2}{10} + \\frac{(4)^2}{10} + \\frac{(1)^2}{10}\\] And if we now square the top halves (the numerators): \\[\\chi^2 = \\frac{25}{10} + \\frac{0}{10} + \\frac{16}{10} + \\frac{1}{10}\\] Then divide the top half by the bottom half for each condition: \\[\\chi^2 = {2.5}+{0}+{1.6}+{0.1}\\] And finally add them altogether \\[\\chi^2 = 4.2\\] So we find that \\(\\chi^2 = 4.2\\) The degrees of freedom in this test is \\(k - 1\\) and given that we have 4 conditions: \\[df = k - 1\\] \\[df = 4 - 1\\] \\[df = 3\\] The effect size A common effect size for the one-sample chi-square test is \\(\\phi\\) (pronounced “ph-aye” and can be written as “phi”). The formula for \\(\\phi\\) is: \\[\\phi = \\sqrt\\frac{\\chi^2}{N}\\] And if we know that \\(\\chi^2 =4.2\\) and that \\(N = 40\\), then putting them into the formula we get: \\[\\phi = \\sqrt\\frac{4.2}{40}\\] \\[\\phi = 0.324037\\] The write-up If we were to look at a critical value look-up table, we would see that the critical value associated with a \\(df = 3\\) at \\(\\alpha = .05\\), to three decimal places, is \\(\\chi^2_{crit} = 7.815\\). As the chi-square value of this test (i.e. \\(\\chi^2 = 4.2\\)) is smaller than \\(\\chi^2_{crit}\\) then we can say that our test is non-significant, and as such would be written up as \\(\\chi^2(df = 3, N = 40) = 4.2,p &gt; .05\\). Finally, if our test was significant then all we need to do is state the condition with the highest frequency (i.e. the mode), which in this case is Condition C 2.3 ChiSquare Look-up Table df \\(\\alpha = .05\\) 1 3.841 2 5.991 3 7.815 4 9.488 5 11.07 6 12.592 7 14.067 8 15.507 9 16.919 10 18.307 "],
["chi-square-cross-tabulation.html", "Chapter 3 Chi-Square Cross-Tabulation 3.1 The Worked Example 3.2 Test Yourself 3.3 ChiSquare Look-up Table", " Chapter 3 Chi-Square Cross-Tabulation 3.1 The Worked Example Here is our data: Groups Yes No A 88 93 B 160 59 We are going to need to know the Column Totals and Row Totals and the Total number of participants (N), so lets calculate them add them to our tables: Group A Row Total = 88 + 93 = 181 Group B Row Total = 160 + 59 = 219 Yes Column Total = 88 + 160 = 248 No Column Total = 93 + 59 = 152 N = 88 + 93 +160 + 59 = 400 And if we add those to our table we see: Groups Yes No Totals A 88 93 181 B 160 59 219 Totals 248 152 400 Now the formula for the chi-square is: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] The Expected values for each condition, in the cross-tabulation in a few ways but the one we will use here is probably the easiest to use and it is: \\[Expected = \\frac{Total_{row} \\times Total_{column}}{N_{total}}\\] This is the same as other versions you might have seen such as: \\[Expected = \\frac{Total_{row}}{N_{total}} \\times \\frac{Total_{column}}{N_{total}} \\times N_{total}\\] The will both give the same result. So using the first approach we would see that the Expected values are: For Group A people that said Yes: \\[Expected_{A-Yes} = \\frac{181 \\times 248}{400} = \\frac{44888}{400} = 112.22\\] For Group A people that said No: \\[Expected_{A-No} = \\frac{181 \\times 152}{400} = \\frac{27512}{400} = 68.78\\] For Group B people that said Yes: \\[Expected_{B-Yes} = \\frac{219 \\times 248}{400} = \\frac{54312}{400} = 135.78\\] For Group B people that said No: \\[Expected_{B-No} = \\frac{219 \\times 152}{400} = \\frac{33288}{400} = 83.22\\] We now have our data, let’s start putting it into the formula, which we said was: \\[\\chi^2 = \\sum\\frac{(Observed - Expected)^2}{Expected}\\] Which really means: \\[\\chi^2 = \\frac{(Observed_{A-Yes} - Expected_{A-Yes})^2}{Expected_{A-Yes}} + \\frac{(Observed_{A-No} - Expected_{A-No})^2}{Expected_{A-No}} + \\\\ \\frac{(Observed_{B-Yes} - Expected_{B-Yes})^2}{Expected_{B-Yes}} + \\frac{(Observed_{B-No} - Expected_{B-No})^2}{Expected_{B-No}}\\] And if we start putting in the values, becomes: \\[\\chi^2 = \\frac{(88 - 112.22)^2}{112.22}+ \\frac{(93 - 68.78)^2}{68.78}+\\frac{(160 - 135.78)^2}{135.78}+\\frac{(59 - 83.22)^2}{83.22}\\] And if we start to tidy those top halves up a little it becomes: \\[\\chi^2 = \\frac{(-24.22)^2}{112.22}+ \\frac{(24.22)^2}{68.78}+\\frac{(24.22)^2}{135.78}+\\frac{(-24.22)^2}{83.22}\\] And now we square those top halves to give: \\[\\chi^2 = \\frac{586.6084}{112.22} + \\frac{586.6084}{68.78} + \\frac{586.6084}{135.78} + \\frac{586.6084}{83.22} \\] And then divide the top halves by the bottom halves \\[\\chi^2 = {5.2273071} + {8.5287642} + {4.3202858} + {7.0488873}\\] And then we sum them altogether to find: \\[\\chi^2 = 25.1252443 \\] Meaning that, rounded to two decimal places, we find \\(\\chi^2 = 25.13\\) Degrees of Freedom The degrees of freedom for the cross-tabulation is calculated as: \\[df = (Rows - 1) \\times (Columns - 1)\\] Which is read as the number of Rows minus 1 times the number of Columns minus 1. If we look at our original data again: Groups Yes No A 88 93 B 160 59 Looking at the table we see we have 2 rows and 2 columns of actual observed data (not looking at the titles and group names), so: \\[Rows - 1 = 2 - 1 = 1\\] And \\[Columns - 1 = 2 - 1 = 1\\] Meaning that: \\[df = (Rows - 1) \\times (Columns - 1)\\] which becomes: \\[df = (2 - 1) \\times (2 - 1)\\] And reduces to: \\[df = (1) \\times (1)\\] leaving us with: \\[df = 1\\] so we see that \\(df = 1\\) The effect size One of the common effect sizes for a cross-tabulation chi-square test is Cramer’s \\(V\\) and is calculated as: \\[V = \\sqrt\\frac{\\chi^2}{N \\times \\min(C-1, R-1)}\\] The key thing to note is \\(\\min(C-1, R-1)\\) which is read as the minimum of EITHER the number of Columns (C) minus 1 OR the number of Rows (R) minus 1; whichever of those two values is smallest. And the minimum of Columns minus 1 or Rows minus 1 is: \\[\\min(C-1, R-1) = \\min( 2 - 1, 2 - 1)\\] \\[\\min(C-1, R-1) = \\min( 1, 1)\\] which gives us: \\[\\min(C-1, R-1) = 1\\] And now we can start completing the Cramer’s \\(V\\) formula as we know: \\(\\chi^2 = 25.13\\) \\(N = 400\\) \\(\\min(C-1, R-1) = 1\\) Giving us: \\[V = \\sqrt\\frac{25.13}{400 \\times 1}\\] And if we deal with the bottom half first we get: \\[V = \\sqrt\\frac{25.13}{400}\\] Then if we divide the top by the bottom we get \\[V = \\sqrt{0.062825}\\] Giving us: \\[V = 0.2506492\\] So we see that, rounded to two decimal places, the effect size is \\(V = 0.25\\) The write-up If we were to look at a critical value look-up table, we would see that the critical value associated with a \\(df = 1\\) at \\(\\alpha = .05\\), to three decimal places, is \\(\\chi^2_{crit} = 3.841\\). As the chi-square value of this test (i.e. \\(\\chi^2 = 25.13\\)) is larger than \\(\\chi^2_{crit}\\) then we can say that our test is significant, and as such would be written up as \\(\\chi^2(df = 1, N = 400) = 25.13,p &lt; .05, V = 0.25\\). If our test was significant then we would go on to carry out the relevant one-sample chi-squares to breakdown how the association manifests itself. The key thing to remember however is that in these one-sample chi-squares, following the cross-tabulation, you MUST use the expected values from the cross-tabulation and do not calculate new expected values. For instance if you were looking at the one-sample chi-square within Group A, then you would use the expected values of \\(Expected_{A-Yes} = 112.22\\) and \\(Expected_{A-No} = 68.78\\) to compare against the observed values of \\(Observed_{A-Yes} = 88\\) and \\(Observed_{A-No} = 93\\) as such: \\[\\chi^2 = \\frac{(88 - 112.22)^2}{112.22}+ \\frac{(93 - 68.78)^2}{68.78}\\] Which if you walk the process through you find: \\[\\chi^2 = 13.7560713\\] Meaning that, rounded to two decimal places, we find \\(\\chi^2 = 13.76\\) To Be Continued 3.2 Test Yourself 3.2.1 DataSet 1 To Be Added 3.3 ChiSquare Look-up Table df \\(\\alpha = .05\\) 1 3.841 2 5.991 3 7.815 4 9.488 5 11.07 6 12.592 7 14.067 8 15.507 9 16.919 10 18.307 "],
["between-subjects-students-t-test.html", "Chapter 4 Between-Subjects Student’s t-test 4.1 The Worked Example 4.2 Test Yourself 4.3 Look-Up table", " Chapter 4 Between-Subjects Student’s t-test between-subjects t-test: Compare two groups or conditions where the participants are different in each group and have not been matched or are only matched on broad demographics, e.g. only age. 4.1 The Worked Example Here is your data: Group N Mean SD A 47 26.45 2.44 B 47 28.38 2.88 Let’s look at the main t-test formula: \\[t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\times \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}}\\] Now, from the table above we know: the mean of Group A is \\(\\bar{X_1} = 26.45\\), the mean of Group B is \\(\\bar{X_2} = 28.38\\), the N of Group A is \\(N_1 = 47\\), and the N of Group B is \\(N_2 = 47\\), which we can put into the equation right now: \\[t = \\frac{26.45 - 28.38}{s_p \\times \\sqrt{\\frac{1}{47} + \\frac{1}{47}}}\\] And now we can see that the only thing we don’t yet know is the pooled standard deviation (\\(s_p\\)). Let’s look at that formula: Calculating the pooled standard deviation \\[s_p = \\sqrt{\\frac{(n_1 -1) \\times s^2_{X_1} + (n_2 -1)\\times s^2_{X_2}}{n_1 + n_2 - 2}}\\] And if we start to fill in some details: \\[s_p = \\sqrt{\\frac{(47 -1) \\times s^2_{X_1} + (47 -1)\\times s^2_{X_2}}{47 + 47 - 2}}\\] Now looking at the formula, it is clear we are missing: \\(s^2_{X_1}\\) - the variance of Group A (could be written as \\(s^2_{A}\\)) \\(s^2_{X_2}\\) - the variance of Group B (could be written as \\(s^2_{B}\\)) What we do know though, from the table, is the standard deviations of both groups (\\(SD_A\\) = 2.44; \\(SD_B\\) = 2.88), and we know that variance of a group is equal to the standard deviation squared. So: \\(s^2_{X_1}\\) = \\(s^2_A\\) = \\(SD_A \\times SD_A\\) = \\(2.44 \\times 2.44\\) = \\(5.9536\\) \\(s^2_{X_2}\\) = \\(s^2_B\\) = \\(SD_B \\times SD_B\\) = \\(2.88 \\times 2.88\\) = \\(8.2944\\) And if we now add those values to our formula we get: \\[s_p = \\sqrt{\\frac{(47 -1) \\times 5.9536 + (47 -1)\\times 8.2944}{47 + 47 - 2}}\\] And we can then start working through the formula, taking each stage in turn to make sure we don’t make mistakes. Let’s get rid of the brackets first: \\[s_p = \\sqrt{\\frac{(46 \\times 5.9536) + (46 \\times 8.2944)}{92}}\\] Now we deal with the multiplications: \\[s_p = \\sqrt{\\frac{273.8656 + 381.5424}{92}}\\] Let’s tidy up that top half of the equation (the numerator): \\[s_p = \\sqrt{\\frac{655.408}{92}}\\] Which if we then divide the numerator by the denominator (the bottom half), and then take the square root of that we get: \\[s_p = \\sqrt{7.124}\\] Giving a pooled standard deviation of: \\[s_p = 2.6690822\\] Meaning that our pooled standard deviation, rounded to three decimal places, is \\(s_p = 2.669\\) and we can now add that to the t-test formula to give us: Calculating the t-value \\[t = \\frac{26.45 - 28.38}{2.669 \\times \\sqrt{\\frac{1}{47} + \\frac{1}{47}}}\\] And again we just start working through the formula. Let’s deal with the fractions relating to sample size first: \\[t = \\frac{26.45 - 28.38}{2.669 \\times \\sqrt{0.0212766 + 0.0212766}}\\] Which we can tidy up a little to give: \\[t = \\frac{-1.93}{2.669 \\times \\sqrt{0.0425532}}\\] And if we sort out the square root on the denominator we are left with: \\[t = \\frac{-1.93}{2.669 \\times 0.2062842}\\] We can then tidy up the denominator to give us: \\[t = \\frac{-1.93}{0.5505727}\\] Which we can finally solve to give us a t-value, rounded to two decimal places, of \\(t = -3.51\\) Degrees of Freedom Great! Now we just need the degrees of freedom where the formula is: \\[df = (n_1 - 1) + (n_2 - 1)\\] And we already know that: the N of Group A is \\(N_1 = 47\\), and the N of Group B is \\(N_2 = 47\\), So putting them into the equation we get: \\[df = (47 - 1) + (47 - 1)\\] \\[df = 46 + 46\\] \\[df = 92\\] Effect Size: Cohen’s d And finally Cohen’s d, the effect size: \\[d = \\frac{2t}{\\sqrt{df}}\\] Which, based on the info above, we know: \\(t = -3.51\\) \\(df = 92\\) Putting them into the formula we get: \\[d = \\frac{2 \\times -3.51}{\\sqrt{92}}\\] And if we tidy the nominator and the denominator we get: \\[d = \\frac{-7.02}{9.591663}\\] Which we can then solve to learn that \\(d = -0.73\\) Determining Significance If we were to look at a critical values look-up table for \\(df = 92\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 1.986\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 3.51\\), is equal to or larger than our \\(t_{crit}\\) then we can say our result is significant, and as such would be written up as t(92) = 3.51, p &lt; .05, d = 0.73 Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 6.959109810^{-4}, and would be written up as p &lt; .001 4.2 Test Yourself 4.2.1 DataSet 1 Here is your data: Group N Mean SD A 48 13.13 2.28 B 48 14.06 2.43 Show me the working and answer Let’s look at the main t-test formula: \\[t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\times \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}}\\] Now, from the table above we know: the mean of Group A is \\(\\bar{X_1} = 13.13\\), the mean of Group B is \\(\\bar{X_2} = 14.06\\), the N of Group A is \\(N_1 = 48\\), and the N of Group B is \\(N_2 = 48\\), which we can put into the equation right now: \\[t = \\frac{13.13 - 14.06}{s_p \\times \\sqrt{\\frac{1}{48} + \\frac{1}{48}}}\\] And now we can see that the only thing we don’t yet know is the pooled standard deviation (\\(s_p\\)). Let’s look at that formula: \\[s_p = \\sqrt{\\frac{(n_1 -1) \\times s^2_{X_1} + (n_2 -1)\\times s^2_{X_2}}{n_1 + n_2 - 2}}\\] And if we start to fill in some details: \\[s_p = \\sqrt{\\frac{(48 -1) \\times s^2_{X_1} + (48 -1)\\times s^2_{X_2}}{48 + 48 - 2}}\\] Now looking at the formula, it is clear we are missing: \\(s^2_{X_1}\\) - the variance of Group A (could be written as \\(s^2_{A}\\)) \\(s^2_{X_2}\\) - the variance of Group B (could be written as \\(s^2_{B}\\)) What we do know though, from the table, is the standard deviations of both groups (\\(SD_A\\) = 2.28; \\(SD_B\\) = 2.43), and we know that variance of a group is equal to the standard deviation squared. So: \\(s^2_{X_1}\\) = \\(s^2_A\\) = \\(SD_A \\times SD_A\\) = \\(2.28 \\times 2.28\\) = \\(5.1984\\) \\(s^2_{X_2}\\) = \\(s^2_B\\) = \\(SD_B \\times SD_B\\) = \\(2.43 \\times 2.43\\) = \\(5.9049\\) And if we now add those values to our formula we get: \\[s_p = \\sqrt{\\frac{(48 -1) \\times 5.1984 + (48 -1)\\times 5.9049}{48 + 48 - 2}}\\] And we can then start working through the formula, taking each stage in turn to make sure we don’t make mistakes. Let’s get rid of the brackets first: \\[s_p = \\sqrt{\\frac{(47 \\times 5.1984) + (47 \\times 5.9049)}{94}}\\] Now we deal with the multiplications: \\[s_p = \\sqrt{\\frac{244.3248 + 277.5303}{94}}\\] Let’s tidy up that top half of the equation (the numerator): \\[s_p = \\sqrt{\\frac{521.8551}{94}}\\] Which if we then divide the numerator by the denominator (the bottom half), and then take the square root of that we get: \\[s_p = \\sqrt{5.55165}\\] Giving a pooled standard deviation of: \\[s_p = 2.356194\\] Meaning that our pooled standard deviation, rounded to three decimal places, is \\(s_p = 2.356\\) and we can now add that to the t-test formula to give us: \\[t = \\frac{13.13 - 14.06}{2.356 \\times \\sqrt{\\frac{1}{48} + \\frac{1}{48}}}\\] And again we just start working through the formula. Let’s deal with the fractions relating to sample size first: \\[t = \\frac{13.13 - 14.06}{2.356 \\times \\sqrt{0.0208333 + 0.0208333}}\\] Which we can tidy up a little to give: \\[t = \\frac{-0.93}{2.356 \\times \\sqrt{0.0416667}}\\] And if we sort out the square root on the denominator we are left with: \\[t = \\frac{-0.93}{2.356 \\times 0.2041241}\\] We can then tidy up the denominator to give us: \\[t = \\frac{-0.93}{0.4809165}\\] Which we can finally solve to give us a t-value, rounded to two decimal places, of \\(t = -1.93\\) Great! Now we just need the degrees of freedom where the formula is: \\[df = (n_1 - 1) + (n_2 - 1)\\] And we already know that: the N of Group A is \\(N_1 = 48\\), and the N of Group B is \\(N_2 = 48\\), So putting them into the equation we get: \\[df = (48 - 1) + (48 - 1)\\] \\[df = 47 + 47\\] \\[df = 94\\] And finally Cohen’s d, the effect size: \\[d = \\frac{2t}{\\sqrt{df}}\\] Which, based on the info above, we know: \\(t = -1.93\\) \\(df = 94\\) Putting them into the formula we get: \\[d = \\frac{2 \\times -1.93}{\\sqrt{94}}\\] And if we tidy the nominator and the denominator we get: \\[d = \\frac{-3.86}{9.6953597}\\] Which we can then solve to learn that \\(d = -0.4\\) Determining Significance If we were to look at a critical values look-up table for \\(df = 94\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 1.986\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 1.93\\), is smaller than our \\(t_{crit}\\) then we can say our result is non-significant, and as such would be written up as t(94) = 1.93, p &gt; .05, d = 0.4 Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0.0566217, and would be written up as p = 0.057 4.2.2 DataSet 2 Here is your data: Group N Mean SD A 17 12.93 1.33 B 17 12.47 1.67 Show me the working and answer Let’s look at the main t-test formula: \\[t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\times \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}}\\] Now, from the table above we know: the mean of Group A is \\(\\bar{X_1} = 12.93\\), the mean of Group B is \\(\\bar{X_2} = 12.47\\), the N of Group A is \\(N_1 = 17\\), and the N of Group B is \\(N_2 = 17\\), which we can put into the equation right now: \\[t = \\frac{12.93 - 12.47}{s_p \\times \\sqrt{\\frac{1}{17} + \\frac{1}{17}}}\\] And now we can see that the only thing we don’t yet know is the pooled standard deviation (\\(s_p\\)). Let’s look at that formula: \\[s_p = \\sqrt{\\frac{(n_1 -1) \\times s^2_{X_1} + (n_2 -1)\\times s^2_{X_2}}{n_1 + n_2 - 2}}\\] And if we start to fill in some details: \\[s_p = \\sqrt{\\frac{(17 -1) \\times s^2_{X_1} + (17 -1)\\times s^2_{X_2}}{17 + 17 - 2}}\\] Now looking at the formula, it is clear we are missing: \\(s^2_{X_1}\\) - the variance of Group A (could be written as \\(s^2_{A}\\)) \\(s^2_{X_2}\\) - the variance of Group B (could be written as \\(s^2_{B}\\)) What we do know though, from the table, is the standard deviations of both groups (\\(SD_A\\) = 1.33; \\(SD_B\\) = 1.67), and we know that variance of a group is equal to the standard deviation squared. So: \\(s^2_{X_1}\\) = \\(s^2_A\\) = \\(SD_A \\times SD_A\\) = \\(1.33 \\times 1.33\\) = \\(1.7689\\) \\(s^2_{X_2}\\) = \\(s^2_B\\) = \\(SD_B \\times SD_B\\) = \\(1.67 \\times 1.67\\) = \\(2.7889\\) And if we now add those values to our formula we get: \\[s_p = \\sqrt{\\frac{(17 -1) \\times 1.7689 + (17 -1)\\times 2.7889}{17 + 17 - 2}}\\] And we can then start working through the formula, taking each stage in turn to make sure we don’t make mistakes. Let’s get rid of the brackets first: \\[s_p = \\sqrt{\\frac{(16 \\times 1.7689) + (16 \\times 2.7889)}{32}}\\] Now we deal with the multiplications: \\[s_p = \\sqrt{\\frac{28.3024 + 44.6224}{32}}\\] Let’s tidy up that top half of the equation (the numerator): \\[s_p = \\sqrt{\\frac{72.9248}{32}}\\] Which if we then divide the numerator by the denominator (the bottom half), and then take the square root of that we get: \\[s_p = \\sqrt{2.2789}\\] Giving a pooled standard deviation of: \\[s_p = 1.5096026\\] Meaning that our pooled standard deviation, rounded to three decimal places, is \\(s_p = 1.51\\) and we can now add that to the t-test formula to give us: \\[t = \\frac{12.93 - 12.47}{1.51 \\times \\sqrt{\\frac{1}{17} + \\frac{1}{17}}}\\] And again we just start working through the formula. Let’s deal with the fractions relating to sample size first: \\[t = \\frac{12.93 - 12.47}{1.51 \\times \\sqrt{0.0588235 + 0.0588235}}\\] Which we can tidy up a little to give: \\[t = \\frac{0.46}{1.51 \\times \\sqrt{0.1176471}}\\] And if we sort out the square root on the denominator we are left with: \\[t = \\frac{0.46}{1.51 \\times 0.3429972}\\] We can then tidy up the denominator to give us: \\[t = \\frac{0.46}{0.5179257}\\] Which we can finally solve to give us a t-value, rounded to two decimal places, of \\(t = 0.89\\) Great! Now we just need the degrees of freedom where the formula is: \\[df = (n_1 - 1) + (n_2 - 1)\\] And we already know that: the N of Group A is \\(N_1 = 17\\), and the N of Group B is \\(N_2 = 17\\), So putting them into the equation we get: \\[df = (17 - 1) + (17 - 1)\\] \\[df = 16 + 16\\] \\[df = 32\\] And finally Cohen’s d, the effect size: \\[d = \\frac{2t}{\\sqrt{df}}\\] Which, based on the info above, we know: \\(t = 0.89\\) \\(df = 32\\) Putting them into the formula we get: \\[d = \\frac{2 \\times 0.89}{\\sqrt{32}}\\] And if we tidy the nominator and the denominator we get: \\[d = \\frac{1.78}{5.6568542}\\] Which we can then solve to learn that \\(d = 0.31\\) Determining Significance If we were to look at a critical values look-up table for \\(df = 32\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 2.037\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 0.89\\), is smaller than our \\(t_{crit}\\) then we can say our result is non-significant, and as such would be written up as t(32) = 0.89, p &gt; .05, d = 0.31 Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0.3801084, and would be written up as p = 0.377 4.2.3 DataSet 3 Here is your data: Group N Mean SD A 23 53.99 4.37 B 23 53.30 3.29 Show me the working and answer Let’s look at the main t-test formula: \\[t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\times \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}}\\] Now, from the table above we know: the mean of Group A is \\(\\bar{X_1} = 53.99\\), the mean of Group B is \\(\\bar{X_2} = 53.3\\), the N of Group A is \\(N_1 = 23\\), and the N of Group B is \\(N_2 = 23\\), which we can put into the equation right now: \\[t = \\frac{53.99 - 53.3}{s_p \\times \\sqrt{\\frac{1}{23} + \\frac{1}{23}}}\\] And now we can see that the only thing we don’t yet know is the pooled standard deviation (\\(s_p\\)). Let’s look at that formula: \\[s_p = \\sqrt{\\frac{(n_1 -1) \\times s^2_{X_1} + (n_2 -1)\\times s^2_{X_2}}{n_1 + n_2 - 2}}\\] And if we start to fill in some details: \\[s_p = \\sqrt{\\frac{(23 -1) \\times s^2_{X_1} + (23 -1)\\times s^2_{X_2}}{23 + 23 - 2}}\\] Now looking at the formula, it is clear we are missing: \\(s^2_{X_1}\\) - the variance of Group A (could be written as \\(s^2_{A}\\)) \\(s^2_{X_2}\\) - the variance of Group B (could be written as \\(s^2_{B}\\)) What we do know though, from the table, is the standard deviations of both groups (\\(SD_A\\) = 4.37; \\(SD_B\\) = 3.29), and we know that variance of a group is equal to the standard deviation squared. So: \\(s^2_{X_1}\\) = \\(s^2_A\\) = \\(SD_A \\times SD_A\\) = \\(4.37 \\times 4.37\\) = \\(19.0969\\) \\(s^2_{X_2}\\) = \\(s^2_B\\) = \\(SD_B \\times SD_B\\) = \\(3.29 \\times 3.29\\) = \\(10.8241\\) And if we now add those values to our formula we get: \\[s_p = \\sqrt{\\frac{(23 -1) \\times 19.0969 + (23 -1)\\times 10.8241}{23 + 23 - 2}}\\] And we can then start working through the formula, taking each stage in turn to make sure we don’t make mistakes. Let’s get rid of the brackets first: \\[s_p = \\sqrt{\\frac{(22 \\times 19.0969) + (22 \\times 10.8241)}{44}}\\] Now we deal with the multiplications: \\[s_p = \\sqrt{\\frac{420.1318 + 238.1302}{44}}\\] Let’s tidy up that top half of the equation (the numerator): \\[s_p = \\sqrt{\\frac{658.262}{44}}\\] Which if we then divide the numerator by the denominator (the bottom half), and then take the square root of that we get: \\[s_p = \\sqrt{14.9605}\\] Giving a pooled standard deviation of: \\[s_p = 3.8678806\\] Meaning that our pooled standard deviation, rounded to three decimal places, is \\(s_p = 3.868\\) and we can now add that to the t-test formula to give us: \\[t = \\frac{53.99 - 53.3}{3.868 \\times \\sqrt{\\frac{1}{23} + \\frac{1}{23}}}\\] And again we just start working through the formula. Let’s deal with the fractions relating to sample size first: \\[t = \\frac{53.99 - 53.3}{3.868 \\times \\sqrt{0.0434783 + 0.0434783}}\\] Which we can tidy up a little to give: \\[t = \\frac{0.69}{3.868 \\times \\sqrt{0.0869565}}\\] And if we sort out the square root on the denominator we are left with: \\[t = \\frac{0.69}{3.868 \\times 0.2948839}\\] We can then tidy up the denominator to give us: \\[t = \\frac{0.69}{1.140611}\\] Which we can finally solve to give us a t-value, rounded to two decimal places, of \\(t = 0.6\\) Great! Now we just need the degrees of freedom where the formula is: \\[df = (n_1 - 1) + (n_2 - 1)\\] And we already know that: the N of Group A is \\(N_1 = 23\\), and the N of Group B is \\(N_2 = 23\\), So putting them into the equation we get: \\[df = (23 - 1) + (23 - 1)\\] \\[df = 22 + 22\\] \\[df = 44\\] And finally Cohen’s d, the effect size: \\[d = \\frac{2t}{\\sqrt{df}}\\] Which, based on the info above, we know: \\(t = 0.6\\) \\(df = 44\\) Putting them into the formula we get: \\[d = \\frac{2 \\times 0.6}{\\sqrt{44}}\\] And if we tidy the nominator and the denominator we get: \\[d = \\frac{1.2}{6.6332496}\\] Which we can then solve to learn that \\(d = 0.18\\) Determining Significance If we were to look at a critical values look-up table for \\(df = 44\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 2.015\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 0.6\\), is smaller than our \\(t_{crit}\\) then we can say our result is non-significant, and as such would be written up as t(44) = 0.6, p &gt; .05, d = 0.18 Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0.551584, and would be written up as p = 0.543 4.3 Look-Up table Remembering that the \\(t_{crit}\\) value is the smallest t-value you need to find a significant effect, find the \\(t_{crit}\\) for your df, assuming \\(\\alpha = .05\\). If the \\(t\\) value you calculated is equal to or larger than \\(t_{crit}\\) then your test is significant. df \\(\\alpha = .05\\) 1 12.706 2 4.303 3 3.182 4 2.776 5 2.571 6 2.447 7 2.365 8 2.306 9 2.262 10 2.228 15 2.131 20 2.086 30 2.042 40 2.021 50 2.009 60 2 70 1.994 80 1.99 90 1.987 100 1.984 "],
["within-subjects-t-test.html", "Chapter 5 Within-Subjects t-test 5.1 The Worked Example 5.2 Test Yourself 5.3 Look-Up table", " Chapter 5 Within-Subjects t-test within-subjects t-test: Compare two conditions where the participants are the same in both conditions (or more rarely are different participants that have been highly matched on a number of demographics such as IQ, reading ability, etc - must be matched on a number of demographics). 5.1 The Worked Example Let’s say that this is our starting data: Participants PreTest PostTest 1 60 68 2 64 75 3 56 62 4 82 85 5 74 73 6 79 85 7 63 64 8 59 59 9 72 73 10 66 70 The first thing we need to do is calculate the difference between the PostTest and the PreTest for each participant, based on \\(D = PostTest - PreTest\\). So for example: Participant 1 would be: 68 - 60 = 8 Participant 2 would be: 75 - 64 = 11 etc And if we do that for each Participant and added a column of the differences (\\(D\\)) then we would see: Participants PreTest PostTest D 1 60 68 8 2 64 75 11 3 56 62 6 4 82 85 3 5 74 73 -1 6 79 85 6 7 63 64 1 8 59 59 0 9 72 73 1 10 66 70 4 Now, the within-subjects t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] We can see that \\(N = 10\\), but we need to calculate \\(\\bar{D}\\) (called D-Bar, the mean of the \\(D\\) column) and \\(SD_{D}\\). Calculating D-bar So the \\(\\bar{D}\\) formula is the same as the mean formula: \\[\\bar{D} = \\frac{\\sum{D}}{N}\\] Where \\(D\\) is \\(PostTest - PreTest\\) for each Participant. Then: \\[\\bar{D} = \\frac{(68 - 60) + (75 - 64) + (62 - 56) + (85 - 82) + (73 - 74) + \\\\ (85- 79) + (64 - 63) + (59 - 59) + (73 - 72) + (70 - 66)}{10}\\] Which if we resolve all the brackets becomes: \\[\\bar{D} = \\frac{8 + 11 + 6 + 3 + -1 + 6 + 1 + 0 + 1 + 2}{10}\\] And if we sum the top half together \\[\\bar{D} = \\frac{39}{10}\\] Leaving us with: \\[\\bar{D} = 3.9\\] So we find that \\(\\bar{D}\\) = 3.9, which is the mean difference between the Post test and Pre test values. The Standard Deviation of D The standard deviation formula is: \\[SD = \\sqrt\\frac{\\sum(X - \\bar{X})^2}{N-1}\\] Which if we translate to using D, becomes: \\[SD_{D} = \\sqrt\\frac{\\sum(D - \\bar{D})^2}{N-1}\\] Then: \\[SD_{D} =\\sqrt\\frac{(8 - 3.9)^2 + (11 - 3.9)^2 + (6 - 3.9)^2 + (3 - 3.9)^2 + (-1 - 3.9)^2 + \\\\ (6 - 3.9)^2 + (1 - 3.9)^2 + (0 - 3.9)^2 + (1 - 3.9)^2 + (4 - 3.9)^2}{10 - 1}\\] And if we start stepping through this analysis by dealing with the brackets: \\[SD_{D} =\\sqrt\\frac{(4.1)^2 + (7.1)^2 + (2.1)^2 + (-0.9)^2 + (-4.9)^2 + \\\\ (2.1)^2 + (-2.9)^2 + (-3.9)^2 + (-2.9)^2 + (0.1)^2}{10 - 1}\\] And then we square those brackets \\[SD_{D} =\\sqrt\\frac{16.81 + 50.41 + 4.41 + 0.81 + 24.01 + 4.41 + 8.41 + 15.21 + 8.41 + 0.01}{10 - 1}\\] And sum up all the values on the top half: \\[SD_{D} =\\sqrt\\frac{132.9}{10 - 1}\\] And then we need to sort out the bottom half \\[SD_{D} =\\sqrt\\frac{132.9}{9}\\] Which by dividing the top half by the bottom half reduces down to: \\[SD_{D} =\\sqrt{14.7666667}\\] And then we finally take the square root, which leaves us with: \\[SD_{D} =3.8427421\\] And so we find that the \\(SD_{D}\\) = 3.8427421 which to two decimal places would be, \\(SD_{D} = 3.84\\) Calculating the t-value And finally the t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] Then if we start filling in the values we know from above we see: \\[t = \\frac{3.9}{\\frac{3.8427421}{\\sqrt{10}}} \\] And if we deal with the square root first: \\[t = \\frac{3.9}{\\frac{3.8427421}{3.1622777}} \\] And divide \\(SD_{D}\\) by \\(\\sqrt{N}\\) - tidying up the bottom of the formula: \\[t = \\frac{3.9}{1.2151817} \\] And then solve for \\(t\\) by dividing the top half by the bottom half gives us: \\[t = 3.2093965 \\] And so, rounding to two decimal places, we find that \\(t = 3.21\\) Degrees of Freedom Great! Now we just need the degrees of freedom where the formula is: \\[df = N - 1\\] And we already know that \\(N= 10\\) and putting them into the equation we get: \\[df = 10 - 1\\] Which reduces to: \\[df = 9\\] Meaning that we find a \\(df = 9\\) Effect Size - Cohen’s d And finally Cohen’s d, the effect size. One of the common formulas based on knowing the t-value and the N is: \\[d = \\frac{t}{\\sqrt{N}}\\] Which, based on the info above, we know: \\(t = 3.21\\) \\(N = 10\\) And putting those into the formula we get: \\[d = \\frac{3.21}{\\sqrt{10}}\\] Which gives us: \\[d = \\frac{3.21}{3.1622777}\\] And so: \\[d = 1.0150911\\] Meaning that the effect size, to two decimal places, is d = 1.01. Determining Significance If we were to look at a critical values look-up table for \\(df = 9\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 2.262\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 3.21\\), is equal to or larger than our \\(t_{crit}\\) then we can say our result is significant, and as such would be written up as t(9) = 3.21, p &lt; .05, d = 1.01. Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0.011, and would be written up as p = 0.011 5.2 Test Yourself 5.2.1 DataSet 1 Let’s say that this is our starting data: Participants PreTest PostTest 1 71 66 2 78 75 3 53 54 4 73 67 5 79 75 6 60 62 7 75 74 8 52 52 9 55 48 10 62 60 Show me the working and answer The first thing we need to do is calculate the difference between the PostTest and the PreTest for each participant, based on \\(D = PostTest - PreTest\\). So for example: Participant 1 would be: 66 - 71 = -5 Participant 2 would be: 75 - 78 = -3 etc And if we do that for each Participant and added a column of the differences (\\(D\\)) then we would see: Participants PreTest PostTest D 1 71 66 -5 2 78 75 -3 3 53 54 1 4 73 67 -6 5 79 75 -4 6 60 62 2 7 75 74 -1 8 52 52 0 9 55 48 -7 10 62 60 -2 Now, the within-subjects t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] We can see that \\(N = 10\\), but we need to calculate \\(\\bar{D}\\) (called D-Bar, the mean of the \\(D\\) column) and \\(SD_{D}\\). Calculating D-bar So the \\(\\bar{D}\\) formula is the same as the mean formula: \\[\\bar{D} = \\frac{\\sum{D}}{N}\\] Where \\(D\\) is \\(PostTest - PreTest\\) for each Participant. Then: \\[\\bar{D} = \\frac{(66 - 71) + (75 - 78) + (54 - 53) + (67 - 73) + (75 - 79) + \\\\ (62- 60) + (74 - 75) + (52 - 52) + (48 - 55) + (60 - 62)}{10}\\] Which if we resolve all the brackets becomes: \\[\\bar{D} = \\frac{-5 + -3 + 1 + -6 + -4 + 2 + -1 + 0 + -7 + 4}{10}\\] And if we sum the top half together \\[\\bar{D} = \\frac{-25}{10}\\] Leaving us with: \\[\\bar{D} = -2.5\\] So we find that \\(\\bar{D}\\) = -2.5, which is the mean difference between the Post test and Pre test values. The Standard Deviation of D The standard deviation formula is: \\[SD = \\sqrt\\frac{\\sum(X - \\bar{X})^2}{N-1}\\] Which if we translate to using D, becomes: \\[SD_{D} = \\sqrt\\frac{\\sum(D - \\bar{D})^2}{N-1}\\] Then: \\[SD_{D} =\\sqrt\\frac{(-5 - -2.5)^2 + (-3 - -2.5)^2 + (1 - -2.5)^2 + (-6 - -2.5)^2 + (-4 - -2.5)^2 + \\\\ (2 - -2.5)^2 + (-1 - -2.5)^2 + (0 - -2.5)^2 + (-7 - -2.5)^2 + (-2 - -2.5)^2}{10 - 1}\\] And if we start stepping through this analysis by dealing with the brackets: \\[SD_{D} =\\sqrt\\frac{(-2.5)^2 + (-0.5)^2 + (3.5)^2 + (-3.5)^2 + (-1.5)^2 + \\\\ (4.5)^2 + (1.5)^2 + (2.5)^2 + (-4.5)^2 + (0.5)^2}{10 - 1}\\] And then we square those brackets \\[SD_{D} =\\sqrt\\frac{6.25 + 0.25 + 12.25 + 12.25 + 2.25 + 20.25 + 2.25 + 6.25 + 20.25 + 0.25}{10 - 1}\\] And sum up all the values on the top half: \\[SD_{D} =\\sqrt\\frac{82.5}{10 - 1}\\] And then we need to sort out the bottom half \\[SD_{D} =\\sqrt\\frac{82.5}{9}\\] Which by dividing the top half by the bottom half reduces down to: \\[SD_{D} =\\sqrt{9.1666667}\\] And then we finally take the square root, which leaves us with: \\[SD_{D} =3.0276504\\] And so we find that the \\(SD_{D}\\) = 3.0276504 which to two decimal places would be, \\(SD_{D} = 3.03\\) Calculating the t-value And finally the t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] Then if we start filling in the values we know from above we see: \\[t = \\frac{-2.5}{\\frac{3.0276504}{\\sqrt{10}}} \\] And if we deal with the square root first: \\[t = \\frac{-2.5}{\\frac{3.0276504}{3.1622777}} \\] And divide \\(SD_{D}\\) by \\(\\sqrt{N}\\) - tidying up the bottom of the formula: \\[t = \\frac{-2.5}{0.9574271} \\] And then solve for \\(t\\) by dividing the top half by the bottom half gives us: \\[t = -2.6111648 \\] And so, rounding to two decimal places, we find that \\(t = -2.61\\) Degrees of Freedom Great! Now we just need the degrees of freedom where the formula is: \\[df = N - 1\\] And we already know that \\(N= 10\\) and putting them into the equation we get: \\[df = 10 - 1\\] Which reduces to: \\[df = 9\\] Meaning that we find a \\(df = 9\\) Effect Size - Cohen’s d And finally Cohen’s d, the effect size. One of the common formulas based on knowing the t-value and the N is: \\[d = \\frac{t}{\\sqrt{N}}\\] Which, based on the info above, we know: \\(t = -2.61\\) \\(N = 10\\) And putting those into the formula we get: \\[d = \\frac{-2.61}{\\sqrt{10}}\\] Which gives us: \\[d = \\frac{-2.61}{3.1622777}\\] And so: \\[d = -0.8253545\\] Meaning that the effect size, to two decimal places, is d = -0.83. Determining Significance If we were to look at a critical values look-up table for \\(df = 9\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 2.262\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 2.61\\), is equal to or larger than our \\(t_{crit}\\) then we can say our result is not significant, and as such would be written up as t(9) = 2.61, p &lt; .05, d = 0.83. Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0.028, and would be written up as p = 0.028 5.2.2 DataSet 2 Let’s say that this is our starting data: Participants PreTest PostTest 1 70 66 2 79 80 3 56 52 4 63 64 5 50 52 6 59 52 7 65 59 8 62 64 9 74 74 10 66 67 Show me the working and answer The first thing we need to do is calculate the difference between the PostTest and the PreTest for each participant, based on \\(D = PostTest - PreTest\\). So for example: Participant 1 would be: 66 - 70 = -4 Participant 2 would be: 80 - 79 = 1 etc And if we do that for each Participant and added a column of the differences (\\(D\\)) then we would see: Participants PreTest PostTest D 1 70 66 -4 2 79 80 1 3 56 52 -4 4 63 64 1 5 50 52 2 6 59 52 -7 7 65 59 -6 8 62 64 2 9 74 74 0 10 66 67 1 Now, the within-subjects t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] We can see that \\(N = 10\\), but we need to calculate \\(\\bar{D}\\) (called D-Bar, the mean of the \\(D\\) column) and \\(SD_{D}\\). Calculating D-bar So the \\(\\bar{D}\\) formula is the same as the mean formula: \\[\\bar{D} = \\frac{\\sum{D}}{N}\\] Where \\(D\\) is \\(PostTest - PreTest\\) for each Participant. Then: \\[\\bar{D} = \\frac{(66 - 70) + (80 - 79) + (52 - 56) + (64 - 63) + (52 - 50) + \\\\ (52- 59) + (59 - 65) + (64 - 62) + (74 - 74) + (67 - 66)}{10}\\] Which if we resolve all the brackets becomes: \\[\\bar{D} = \\frac{-4 + 1 + -4 + 1 + 2 + -7 + -6 + 2 + 0 + 0}{10}\\] And if we sum the top half together \\[\\bar{D} = \\frac{-14}{10}\\] Leaving us with: \\[\\bar{D} = -1.4\\] So we find that \\(\\bar{D}\\) = -1.4, which is the mean difference between the Post test and Pre test values. The Standard Deviation of D The standard deviation formula is: \\[SD = \\sqrt\\frac{\\sum(X - \\bar{X})^2}{N-1}\\] Which if we translate to using D, becomes: \\[SD_{D} = \\sqrt\\frac{\\sum(D - \\bar{D})^2}{N-1}\\] Then: \\[SD_{D} =\\sqrt\\frac{(-4 - -1.4)^2 + (1 - -1.4)^2 + (-4 - -1.4)^2 + (1 - -1.4)^2 + (2 - -1.4)^2 + \\\\ (-7 - -1.4)^2 + (-6 - -1.4)^2 + (2 - -1.4)^2 + (0 - -1.4)^2 + (1 - -1.4)^2}{10 - 1}\\] And if we start stepping through this analysis by dealing with the brackets: \\[SD_{D} =\\sqrt\\frac{(-2.6)^2 + (2.4)^2 + (-2.6)^2 + (2.4)^2 + (3.4)^2 + \\\\ (-5.6)^2 + (-4.6)^2 + (3.4)^2 + (1.4)^2 + (2.4)^2}{10 - 1}\\] And then we square those brackets \\[SD_{D} =\\sqrt\\frac{6.76 + 5.76 + 6.76 + 5.76 + 11.56 + 31.36 + 21.16 + 11.56 + 1.96 + 5.76}{10 - 1}\\] And sum up all the values on the top half: \\[SD_{D} =\\sqrt\\frac{108.4}{10 - 1}\\] And then we need to sort out the bottom half \\[SD_{D} =\\sqrt\\frac{108.4}{9}\\] Which by dividing the top half by the bottom half reduces down to: \\[SD_{D} =\\sqrt{12.0444444}\\] And then we finally take the square root, which leaves us with: \\[SD_{D} =3.4705107\\] And so we find that the \\(SD_{D}\\) = 3.4705107 which to two decimal places would be, \\(SD_{D} = 3.47\\) Calculating the t-value And finally the t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] Then if we start filling in the values we know from above we see: \\[t = \\frac{-1.4}{\\frac{3.4705107}{\\sqrt{10}}} \\] And if we deal with the square root first: \\[t = \\frac{-1.4}{\\frac{3.4705107}{3.1622777}} \\] And divide \\(SD_{D}\\) by \\(\\sqrt{N}\\) - tidying up the bottom of the formula: \\[t = \\frac{-1.4}{1.0974718} \\] And then solve for \\(t\\) by dividing the top half by the bottom half gives us: \\[t = -1.2756592 \\] And so, rounding to two decimal places, we find that \\(t = -1.28\\) Degrees of Freedom Great! Now we just need the degrees of freedom where the formula is: \\[df = N - 1\\] And we already know that \\(N= 10\\) and putting them into the equation we get: \\[df = 10 - 1\\] Which reduces to: \\[df = 9\\] Meaning that we find a \\(df = 9\\) Effect Size - Cohen’s d And finally Cohen’s d, the effect size. One of the common formulas based on knowing the t-value and the N is: \\[d = \\frac{t}{\\sqrt{N}}\\] Which, based on the info above, we know: \\(t = -1.28\\) \\(N = 10\\) And putting those into the formula we get: \\[d = \\frac{-1.28}{\\sqrt{10}}\\] Which gives us: \\[d = \\frac{-1.28}{3.1622777}\\] And so: \\[d = -0.4047715\\] Meaning that the effect size, to two decimal places, is d = -0.4. Determining Significance If we were to look at a critical values look-up table for \\(df = 9\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 2.262\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 1.28\\), is smaller than our \\(t_{crit}\\) then we can say our result is not significant, and as such would be written up as t(9) = 1.28, p &gt; .05, d = 0.4. Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0.234, and would be written up as p = 0.234 5.2.3 DataSet 3 Let’s say that this is our starting data: Participants PreTest PostTest 1 78 71 2 60 53 3 76 73 4 56 52 5 62 56 6 73 69 7 61 58 8 70 65 9 52 52 10 65 64 Show me the working and answer The first thing we need to do is calculate the difference between the PostTest and the PreTest for each participant, based on \\(D = PostTest - PreTest\\). So for example: Participant 1 would be: 71 - 78 = -7 Participant 2 would be: 53 - 60 = -7 etc And if we do that for each Participant and added a column of the differences (\\(D\\)) then we would see: Participants PreTest PostTest D 1 78 71 -7 2 60 53 -7 3 76 73 -3 4 56 52 -4 5 62 56 -6 6 73 69 -4 7 61 58 -3 8 70 65 -5 9 52 52 0 10 65 64 -1 Now, the within-subjects t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] We can see that \\(N = 10\\), but we need to calculate \\(\\bar{D}\\) (called D-Bar, the mean of the \\(D\\) column) and \\(SD_{D}\\). Calculating D-bar So the \\(\\bar{D}\\) formula is the same as the mean formula: \\[\\bar{D} = \\frac{\\sum{D}}{N}\\] Where \\(D\\) is \\(PostTest - PreTest\\) for each Participant. Then: \\[\\bar{D} = \\frac{(71 - 78) + (53 - 60) + (73 - 76) + (52 - 56) + (56 - 62) + \\\\ (69- 73) + (58 - 61) + (65 - 70) + (52 - 52) + (64 - 65)}{10}\\] Which if we resolve all the brackets becomes: \\[\\bar{D} = \\frac{-7 + -7 + -3 + -4 + -6 + -4 + -3 + -5 + 0 + 6}{10}\\] And if we sum the top half together \\[\\bar{D} = \\frac{-40}{10}\\] Leaving us with: \\[\\bar{D} = -4\\] So we find that \\(\\bar{D}\\) = -4, which is the mean difference between the Post test and Pre test values. The Standard Deviation of D The standard deviation formula is: \\[SD = \\sqrt\\frac{\\sum(X - \\bar{X})^2}{N-1}\\] Which if we translate to using D, becomes: \\[SD_{D} = \\sqrt\\frac{\\sum(D - \\bar{D})^2}{N-1}\\] Then: \\[SD_{D} =\\sqrt\\frac{(-7 - -4)^2 + (-7 - -4)^2 + (-3 - -4)^2 + (-4 - -4)^2 + (-6 - -4)^2 + \\\\ (-4 - -4)^2 + (-3 - -4)^2 + (-5 - -4)^2 + (0 - -4)^2 + (-1 - -4)^2}{10 - 1}\\] And if we start stepping through this analysis by dealing with the brackets: \\[SD_{D} =\\sqrt\\frac{(-3)^2 + (-3)^2 + (1)^2 + (0)^2 + (-2)^2 + \\\\ (0)^2 + (1)^2 + (-1)^2 + (4)^2 + (3)^2}{10 - 1}\\] And then we square those brackets \\[SD_{D} =\\sqrt\\frac{9 + 9 + 1 + 0 + 4 + 0 + 1 + 1 + 16 + 9}{10 - 1}\\] And sum up all the values on the top half: \\[SD_{D} =\\sqrt\\frac{50}{10 - 1}\\] And then we need to sort out the bottom half \\[SD_{D} =\\sqrt\\frac{50}{9}\\] Which by dividing the top half by the bottom half reduces down to: \\[SD_{D} =\\sqrt{5.5555556}\\] And then we finally take the square root, which leaves us with: \\[SD_{D} =2.3570226\\] And so we find that the \\(SD_{D}\\) = 2.3570226 which to two decimal places would be, \\(SD_{D} = 2.36\\) Calculating the t-value And finally the t-test formula is: \\[t = \\frac{\\bar{D}}{\\frac{SD_{D}}{\\sqrt{N}}}\\] Then if we start filling in the values we know from above we see: \\[t = \\frac{-4}{\\frac{2.3570226}{\\sqrt{10}}} \\] And if we deal with the square root first: \\[t = \\frac{-4}{\\frac{2.3570226}{3.1622777}} \\] And divide \\(SD_{D}\\) by \\(\\sqrt{N}\\) - tidying up the bottom of the formula: \\[t = \\frac{-4}{0.745356} \\] And then solve for \\(t\\) by dividing the top half by the bottom half gives us: \\[t = -5.3665631 \\] And so, rounding to two decimal places, we find that \\(t = -5.37\\) Degrees of Freedom Great! Now we just need the degrees of freedom where the formula is: \\[df = N - 1\\] And we already know that \\(N= 10\\) and putting them into the equation we get: \\[df = 10 - 1\\] Which reduces to: \\[df = 9\\] Meaning that we find a \\(df = 9\\) Effect Size - Cohen’s d And finally Cohen’s d, the effect size. One of the common formulas based on knowing the t-value and the N is: \\[d = \\frac{t}{\\sqrt{N}}\\] Which, based on the info above, we know: \\(t = -5.37\\) \\(N = 10\\) And putting those into the formula we get: \\[d = \\frac{-5.37}{\\sqrt{10}}\\] Which gives us: \\[d = \\frac{-5.37}{3.1622777}\\] And so: \\[d = -1.6981431\\] Meaning that the effect size, to two decimal places, is d = -1.7. Determining Significance If we were to look at a critical values look-up table for \\(df = 9\\) and \\(\\alpha = .05\\) (two-tailed), we would see that the critical value is \\(t_{crit} = 2.262\\). Given that our t-value, ignoring polarity and just looking at the absolute value, so \\(t = 5.37\\), is equal to or larger than our \\(t_{crit}\\) then we can say our result is not significant, and as such would be written up as t(9) = 5.37, p &lt; .05, d = 1.7. Remember: If you were writing this up as a report, and analysis the data in R, then you would see the p-value was actually p = 0, and would be written up as p &lt; .001 5.3 Look-Up table Remembering that the \\(t_{crit}\\) value is the smallest t-value you need to find a significant effect, find the \\(t_{crit}\\) for your df, assuming \\(\\alpha = .05\\). If the \\(t\\) value you calculated is equal to or larger than \\(t_{crit}\\) then your test is significant. df \\(\\alpha = .05\\) 1 12.706 2 4.303 3 3.182 4 2.776 5 2.571 6 2.447 7 2.365 8 2.306 9 2.262 10 2.228 15 2.131 20 2.086 30 2.042 40 2.021 50 2.009 60 2 70 1.994 80 1.99 90 1.987 100 1.984 "],
["correlation-pearson.html", "Chapter 6 Correlation: Pearson 6.1 The Worked Example 6.2 Look-Up table", " Chapter 6 Correlation: Pearson The Pearson correlation otherwise known as the Pearson Product-Moment correlation measures the relationshop between two variables when that relationship is monotonic and linear. To go a step further, in relating it measures the covariance between two variables, or the shared variance, and then standardises that measure between the values of -1 and 1 with -1 being the perfect negative relationship and 1 being the perfect positive relationship. 6.1 The Worked Example Let’s say that this is our starting data: Participants HeadSize IQ 1 50.8 107 2 63.5 121 3 45.7 106 4 25.4 72 5 29.2 85 6 49.5 105 7 38.1 93 8 30.5 88 9 35.6 97 10 58.4 123 and the formula for the Pearson correlation is: \\[r_{(x,y)} = \\frac{cov(x,y)}{\\sigma_{x}\\sigma_{y}}\\] Meaning that, assuming we consider HeadSize as variable \\(x\\) and IQ as variable \\(y\\), to determine the correlation between HeadSize and IQ (\\(r_{(HeadSize, IQ)}\\)) we first need to know the covariance between HeadSize and IQ, (\\(cov(HeadSize, IQ)\\)), as well as the standard deviation of HeadSize (\\(\\sigma_{HeadSize}\\)) and the standard deviation of IQ (\\(\\sigma_{IQ}\\)). That is what we are going to look at here. We will run it all through from start to finish, assuming all we know are the values. If you know certain aspects like the covariance and the standard deviations then you can skip to just running the correlation but it is good to see the analysis in its entirety. Covariance The formula for the covariance between two continuous variables is: \\[cov(x,y) = \\frac{\\sum_i^n(x_i - \\overline{x})(y_i - \\overline{y})}{n-1}\\] where the \\(\\sum_i^n\\) translates as sum (i.e. add up) the values of \\((x_i - \\overline{x})(y_i - \\overline{y})\\) for each pair of observations in your data. But to do that we first need to know the mean of x (\\(\\overline{x}\\)) and the mean of y, (\\(\\overline{y}\\)). Now we know that the mean of a value is the sum of all the values of that variable divided by the number of values - also written as: \\[\\overline{x} = \\frac{\\sum_i^n x_i}{n}\\]. So the mean of x (i.e. HeadSize) is: \\[\\overline{x} = \\frac{50.8 + 63.5 + 45.7 + 25.4 + 29.2 + 49.5 + 38.1 + 30.5 + 35.6 + 58.4}{10}\\] Which reduces to: \\[\\overline{x} = \\frac{426.7}{10} = 42.67\\] And if we repeat the same procedure for y (i.e. IQ), we get: \\[\\overline{y} = \\frac{107 + 121 + 106 + 72 + 85 + 105 + 93 + 88 + 97 + 123}{10}\\] Which reduces to: \\[\\overline{y} = \\frac{997}{10} = 99.7\\] Ok great so we now have enough information to start filling in the covariance formula. First, however, before looking at it for all participants, let’s look at this part, \\((x_i - \\overline{x})(y_i - \\overline{y})\\), for one participant just to make sure we understand what we are doing. And one thing to remember is that two sets of parentheses (i.e. brackets) side-by-side like that means to multiply them together. So, if we take Participant 1, then we see that: The mean of x is \\(\\overline{x} = 42.67\\) The mean of y is \\(\\overline{y} = 99.7\\) They have a HeadSize (x) of \\(x_1\\) = 50.8 They have an IQ (y) of \\(y_1\\) = 107 And note that we are now saying \\(x_1\\) and \\(y_1\\), and no longer \\(x_i\\) and \\(y_i\\), because we know we are specifically dealing with the 1st Participant. If it was the 2nd Participants we would write \\(x_2\\) and \\(y_2\\), and the 8th would be \\(x_8\\) and \\(y_8\\), etc Putting all that into the formula for Participant 1 (P1) would give us: P1 = \\((x_1 - \\overline{x})(y_1 - \\overline{y}) = (50.8 - 42.67)\\times(107 - 99.7) = 8.13 \\times 7.3 = 59.349\\) and we can do that for all participants which would look like: P2 = \\((x_2 - \\overline{x})(y_2 - \\overline{y}) = (63.5 - 42.67)\\times(121 - 99.7) = 20.83\\times21.3 = 443.679\\) P3 = \\((x_3 - \\overline{x})(y_3 - \\overline{y}) = (45.7 - 42.67)\\times(106 - 99.7) = 3.03\\times6.3 = 19.089\\) P4 = \\((x_4 - \\overline{x})(y_4 - \\overline{y}) = (25.4 - 42.67)\\times(72 - 99.7) = -17.27\\times-27.7 = 478.379\\) P5 = \\((x_5 - \\overline{x})(y_5 - \\overline{y}) = (29.2 - 42.67)\\times(85 - 99.7) = -13.47\\times-14.7 = 198.009\\) P6 = \\((x_6 - \\overline{x})(y_6 - \\overline{y}) = (49.5 - 42.67)\\times(105 - 99.7) = 6.83\\times5.3 = 36.199\\) P7 = \\((x_7 - \\overline{x})(y_7 - \\overline{y}) = (38.1 - 42.67)\\times(93 - 99.7) = -4.57\\times-6.7 = 30.619\\) P8 = \\((x_8 - \\overline{x})(y_8 - \\overline{y}) = (30.5 - 42.67)\\times(88 - 99.7) = -12.17\\times-11.7 = 142.389\\) P9 = \\((x_9 - \\overline{x})(y_9 - \\overline{y}) = (35.6 - 42.67)\\times(97 - 99.7) = -7.07\\times-2.7 = 19.089\\) P10 = \\((x_10 - \\overline{x})(y_10 - \\overline{y}) = (58.4 - 42.67)\\times(123 - 99.7) = 15.73\\times23.3 = 366.509\\) And if we remember that the \\(\\sum_i^n\\) part of the formula means that the formula, for this example with 10 participants, is really \\[cov(x,y) = \\frac{(x_1 - \\overline{x})(y_1 - \\overline{y})+(x_2 - \\overline{x})(y_2 - \\overline{y})+(x_3 - \\overline{x})(y_3 - \\overline{y})+(x_4 - \\overline{x})(y_4 - \\overline{y})+\\\\(x_5 - \\overline{x})(y_5 - \\overline{y})+(x_6 - \\overline{x})(y_6 - \\overline{y})+(x_7 - \\overline{x})(y_7 - \\overline{y})+(x_8 - \\overline{x})(y_8 - \\overline{y})+\\\\(x_9 - \\overline{x})(y_9 - \\overline{y})+(x_{10} - \\overline{x})(y_{10} - \\overline{y})}{n-1}\\] Then that becomes: \\[cov(x,y) = \\frac{((50.8 - 42.67)\\times(107 - 99.7)) + ((63.5 - 42.67)\\times(121 - 99.7)) +\\\\ ((45.7 - 42.67)\\times(106 - 99.7)) + ((25.4 - 42.67)\\times(72 - 99.7)) +\\\\ ((29.2 - 42.67)\\times(85 - 99.7)) + ((49.5 - 42.67)\\times(105 - 99.7)) + \\\\((38.1 - 42.67)\\times(93 - 99.7)) + ((30.5 - 42.67)\\times(88 - 99.7)) + \\\\((35.6 - 42.67)\\times(97 - 99.7)) + ((58.4 - 42.67)\\times(123 - 99.7))}{n-1}\\] and, remembering the rules of BODMAS/PEMDAS, reduces to: \\[cov(x,y) = \\frac{59.349 + 443.679 + 19.089 + 478.379 + 198.009\\\\+ 36.199 + 30.619 + 142.389 + 19.089 + 366.509}{n-1}\\] And all these values match the value we calculated above, so we know that is correct. Then if we know n = 10 as that is the number of participants, and so the bottom of the formula becomes n = 10 - 1 = 9, we can reduce all of the above to: \\[cov(x,y) = \\frac{1793.31}{9}\\] Leaving us with a covariance between HeadSize (\\(x\\)) and IQ (\\(y\\)) of: \\[cov(x,y) = 199.2566667\\] Great! Now if we look back at the formula for the Pearson correlation, we see: \\[r_{(x,y)} = \\frac{cov(x,y)}{\\sigma_{x}\\sigma_{y}}\\] So the covariance of x and y, divided by the product (multiply them together) of the standard deviation of x \\(\\sigma_{x}\\) (pronounced sigma-x) and the standard deviation of y \\(\\sigma_{y}\\) (pronounced sigma-y). Remember that \\(\\sigma\\) can also be written as \\(S\\) or \\(SD\\) and is the symbol for the Standard Deviation and that \\(\\sigma^2\\) (pronounced sigma-squared) is the symbol for the variance. So in reality, the pearson correlation between Head Size and IQ is written as: \\[r_{(HeadSize,IQ)} = \\frac{cov(HeadSize,IQ)}{\\sigma_{HeadSize}\\times\\sigma_{IQ}}\\] And if we known from above that \\(cov(HeadSize, IQ)\\) = 199.26 (to two decimal places), now all we need is the standard deviation of Headsize (\\(\\sigma_{HeadSize}\\)) and the standard deviation of IQ (\\(\\sigma_{IQ}\\)). We cover standard deviation calculations in the Descriptives chapter, but to briefly recap, the formula is: \\[\\sigma = \\sqrt\\frac{\\sum_i^n(x_{i} - \\overline{x})^2}{n-1}\\] Which for HeadSize would look like: \\[\\sigma_{HeadSize} = \\sqrt\\frac{(50.8 - 42.67)^2 + (63.5 - 42.67)^2 +(45.7 - 42.67)^2 +(25.4 - 42.67)^2 +\\\\(29.2 - 42.67)^2 +(49.5 - 42.67)^2 +(38.1 - 42.67)^2 +(30.5 - 42.67)^2 +\\\\(35.6 - 42.67)^2 +(58.4 - 42.67)^2}{10-1}\\] Which reduces to: \\[\\sigma_{HeadSize} = \\sqrt\\frac{66.0969 + 433.8889 +9.1809 +298.2529 +\\\\181.4409 +46.6489 +20.8849 +148.1089 +\\\\49.9849 +247.4329 }{9}\\] Giving us: \\[\\sigma_{HeadSize} = \\sqrt\\frac{1501.921}{9} = \\sqrt{166.8801111} = 12.9182085\\] And if we repeat that process for the standard deviation of IQ (\\(\\sigma_{IQ}\\)) then it would be: \\[\\sigma_{IQ} = \\sqrt\\frac{(107 - 99.7)^2 + (121 - 99.7)^2 +(106 - 99.7)^2 +(72 - 99.7)^2 +(85 - 99.7)^2 +\\\\(105 - 99.7)^2 +(93 - 99.7)^2 +(88 - 99.7)^2 +(97 - 99.7)^2 +(123 - 99.7)^2}{10-1}\\] Which reduces to: \\[\\sigma_{IQ} = \\sqrt\\frac{53.29 + 453.69 +39.69 +767.29 +216.09 +\\\\28.09 +44.89 +136.89 +7.29 +542.89 }{9}\\] Giving us: \\[\\sigma_{IQ} = \\sqrt\\frac{2290.1}{9} = \\sqrt{254.4555556} = 15.9516631\\] And so if we know the following, \\(cov(x,y)\\) = \\(cov(HeadSpace, IQ)\\) = 199.26 \\(\\sigma_{x}\\) = \\(\\sigma_{HeadSpace}\\) = 12.92 \\(\\sigma_{y}\\) = \\(\\sigma_{IQ}\\) = 15.95 Then our correlation value is: \\[\\begin{align*} r_{(HeadSize,IQ)} &amp;= \\frac{cov(HeadSize,IQ)}{\\sigma_{HeadSize}\\times\\sigma_{IQ}} \\\\ \\\\ &amp;= \\frac{199.26}{12.92 \\times 15.95} \\\\ \\\\ &amp;= \\frac{199.26}{206.074} \\\\ \\\\ &amp;= 0.9669342 \\end{align*}\\] Meaning that we would have a correlation value of, to three decimal places, \\(r\\) = 0.967, which, following Cohen’s (1988) proposed cut-offs shown below, would be considered to show a strong positive relationship. Remember that these cut-offs are just guides so a value just below a cut-off, e.g. r = .48, would probably be described as, for example, a strong positive relationship. Relationship Description (Cohen, 1988) r-value Strong positive relationship r = .5 Medium positive relationship r = .3 Weak positive relationship r = .1 No relationship r = 0 Weak negative relationship r = -.1 Medium negative relationship r = -.3 Strong negative relationship r = -.5 Degrees of Freedom Great! Now, finally, to determine significance we first need the degrees of freedom for the test. The degrees of freedom formula is: \\[df = N - 2\\] And we already know that \\(N= 10\\) and putting them into the equation we get: \\[df = 10 - 2\\] Which reduces to: \\[df = 8\\] Meaning that we find a \\(df = 8\\) Writing up The standard APA format for writing up a Pearson correlation is usually: r(df) = r-value, p = p-value From above we know the df and we know the r-value and what we need now is the significance. Determining Significance If we were to look at a critical values look-up table for \\(df = 8\\) and \\(\\alpha = .05\\), we would see that the critical value is \\(r_{crit} = 0.632\\). Given that our r-value, ignoring polarity and just looking at the absolute value, so \\(r = 0.967\\), is equal to or larger than our \\(r_{crit}\\) then we can say our result is significant, and as such would be written up as r(8) = 0.967, p &lt; .05. Remember: If you were writing this up as a report, and analysed the data in R, then you would see the p-value was actually p = 0.000005014954, and would be written up as p &lt; .001 6.2 Look-Up table Remembering that the \\(r_{crit}\\) value is the smallest r-value you need to find a significant effect, find the \\(r_{crit}\\) value for your df, assuming \\(\\alpha = .05\\). If the \\(r\\)-value you calculated is equal to or larger than \\(r_{crit}\\) value then your test is significant. df \\(\\alpha = .05\\) 1 0.997 2 0.95 3 0.878 4 0.811 5 0.754 6 0.707 7 0.666 8 0.632 9 0.602 10 0.576 15 0.482 20 0.423 30 0.349 40 0.304 50 0.273 60 0.25 70 0.232 80 0.217 90 0.205 100 0.195 "],
["simple-linear-regression.html", "Chapter 7 Simple Linear Regression 7.1 The slope: 7.2 The intercept 7.3 Making a Prediction 7.4 The R-squared 7.5 The Write-up", " Chapter 7 Simple Linear Regression Simple Linear Regression is analytical method that looks to model the relationship between an outcome variable and one explanatory predictor variables. For example, thinking about the data that we used for the Pearson Correlation analysis in this book, say instead of asking is HeadSize and IQ related, we could ask can you reliably predict IQ scores (our outcome or dependent variable) from HeadSize measurements (our predictor or independent variable), with the hypothesis of, “We predict that a linear model based on head size, as measured in cms, will significantly predict IQ scores as measured on a standard IQ test”. We are going to look at this example to walk through some of the analysis but lets first remind ourselves of the data. Participants HeadSize IQ 1 50.8 107 2 63.5 121 3 45.7 106 4 25.4 72 5 29.2 85 6 49.5 105 7 38.1 93 8 30.5 88 9 35.6 97 10 58.4 123 Now in order to determine how well Head Size predicts IQ we need to know the underlying formula for this type of analysis and use that to calculate elements such as the slope of the relationship and the intercept. You might remember from previous years that a line in a two-dimensional coordinate system (i.e. two dimensional space made of variables X and Y) can be described by a linear equation (model) of the form: \\[Y = b_{0} + b_{1}X + error\\] Where: \\(Y\\) is the value on the outcome variable Y \\(X\\) is the value on the predictor variable X \\(b_{0}\\) is the intercept (or regression constant) – pronounced beta-zero - and stated as the value of \\(Y\\) when \\(X = 0\\) when X = 0 \\(b_{1}\\) is the slope (or regression coefficient) – pronounced beta-one - and stated as the change in \\(Y\\) associated with a one-unit increase in \\(X\\) \\(error\\) is a measure of your residuals which are the difference between an actual value and a predicted value Before getting into the full simple linear regression however we will look at making predictions from this linear model formula using the slope and the intercept, based on the following equations. the slope: \\[b_{1} = \\frac{cov_{(x, y)}}{s^2_{x}}\\] Or alternatively it can be written as, \\[b_{1} = r_{(x,y)} \\times \\frac{s_{y}}{s_{x}}\\] the intercept \\[b_{0} = \\overline{y} - b_{1} \\times \\overline{x}\\] So lets spend some time looking at those two equations in turn. 7.1 The slope: As above, the formula for the slope is: \\[b_{1} = \\frac{cov_{(x, y)}}{s^2_{x}}\\] which is stated as the covariance of x and y (\\(cov_{(x,y)}\\)) divided by the variance of x (\\(s^2_{x}\\)). If we translate that into outcome and predictors then, given our Head Size is the predictor (\\(x\\)) and the IQ is the outcome (\\(y\\)), that becomes: \\[b_{1} = \\frac{cov_{(HeadSize, IQ)}}{s^2_{HeadSize}}\\] But this can also be written in terms of the correlation between the two variables as opposed to the covariance, and this would look like: \\[b_{1} = r_{(x,y)} \\times \\frac{s_{y}}{s_{x}}\\] Which is stated as the correlation between x and y (\\(r_{(x,y)}\\)) multiplied by the standard deviation of y (\\(s_{y}\\)) divided by the standard deviation of x (\\(s_{x}\\)). [Remember that \\(s\\) is the standard deviation of a variable, and \\(s^2\\) is the variance of the variable.] And again we can translate that back into terms of HeadSize (\\(x\\)) and IQ (\\(y\\)) as: \\[b_{1} = r \\times \\frac{s_{IQ}}{s_{HeadSize}}\\] Now if we think back to the correlation chapter we actually do know all the values we need, or at least can calculate them. Here is what we know from the correlation chapter: \\(cov_{(x,y)}\\) = 199.26 \\(s_{HeadSize}\\) = 12.92 \\(s_{IQ}\\) = 15.95 \\(r_{(x,y)}\\) = 0.967 And from that we can calculate the variance of HeadSize (\\(s^2_{HeadSize}\\)) and IQ (\\(s^2_{IQ}\\)), based on \\(s^2 = s \\times s\\), meaning: \\(s^2_{HeadSize}\\) = \\(12.92 \\times 12.92\\) = 166.9264 \\(s^2_{IQ}\\) = \\(15.95 \\times 15.95\\) = 254.4025 So we could really use either formula for the slope as we have all the information, but for now let’s use: \\[b_{1} = r \\times \\frac{s_{IQ}}{s_{HeadSize}}\\] And if we start to feed in the values, remembering that the IQ is (\\(y\\) - the outcome) and HeadSize is (\\(x\\) - the predictor) we get: \\[b_{1} = 0.967 \\times \\frac{15.95}{12.92}\\] And then if we sort out the fraction first, that becomes: \\[b_{1} = 0.967 \\times 1.2345201\\] Which leads down to: \\[b_{1} = 1.193781\\] Giving a slope of \\(b_{1}\\) = 1.194, to three decimal places, meaning that for a 1 unit change in \\(x\\) we get a 1.194 unit change in \\(y\\). Or in other words, for a 1 unit change in HeadSize we get a 1.194 unit change in IQ. 7.2 The intercept Great. So now we know the slope of the regression line between HeadSize and IQ, what we need next is the intercept. The formula for the intercept, as above, is: \\[b_{0} = \\overline{y} - b_{1} \\times \\overline{x}\\] Which is stated as the mean of y (\\(\\overline{y}\\) - in this case IQ, our outcome) minus the slope (\\(b_{1}\\)) multiplied by the mean of x (\\(\\overline{x}\\) - in this case HeadSize, our predictor). Looking back at the correlation chapter, and above, we know: \\(b_{1}\\) = 1.194 \\(\\overline{x}\\) = 42.67 \\(\\overline{y}\\) = 99.7 And if we substitute those values into the formula we get: \\[b_{0} = 99.7 - 1.194 \\times 42.67\\] Remembering BODMAS we need to do the multiplication first, which gives us: \\[b_{0} = 99.7 - 50.94798\\] And we then complete the formula by doing the subtraction giving us: \\[b_{0} = 48.75202\\] Meaning that we have an intercept of \\(b_{0}\\) = 48.75, to two decimal places. And if we remember that the intercept is the value of \\(y\\) when \\(x\\) is zero, then this tells us that when \\(x\\) = 0, the value of \\(y\\) is \\(y\\) = 48.75. Or stated in terms of HeadSize and IQ, when Headsize is 0, then IQ is 48.75. 7.3 Making a Prediction Now based on the information we have gained above we can actually use that to make predictions about a person we have not measured based on the formula: \\[\\hat{Y} = b_{0} + b_{1}X\\] Well technically it is \\[\\hat{Y} = b_{0} + b_{1}X + error\\] But we will disregard the error for now. Also you might have noticed the small hat about the \\(Y\\) making it \\(\\hat{Y}\\) (pronounced Y-hat). This means we are making a prediction of Y (\\(\\hat{Y}\\)) as opposed to an actually measured (i.e. observed) value (\\(Y\\)). Now let’s say we want to make a prediction about someone who has a HeadSize of 60.1. Then, using the information above, we know: the slope, \\(b_{1}\\) = 1.194 the intercept, \\(b_{0}\\) = 48.75 and the HeadSize, \\(X\\) = 60.1 If we put all that into the formula we would get: \\[\\hat{Y} = 48.75 + 1.194 \\times 60.1\\] And if we start to work that through, dealing with the multiplication first, we see: \\[\\hat{Y} = 48.75 + 71.7594\\] Which then becomes: \\[\\hat{Y} = 120.5094\\] Giving a predicted value of \\(\\hat{Y}\\) = 120.51, to two decimal places. Meaning that for a participant with a HeadSize of 60.1 we would predict an IQ of 120.51. And you can use the same approach for any number of predictions with this model. Say you measured someone with a HeadSize of 59.4 cm, then they would be: \\[\\hat{Y} = 48.75 + 1.194 \\times 59.4 = 119.6736\\] Meaning that for a participant with a HeadSize of 59.4 we would predict an IQ of 119.67. 7.4 The R-squared Obviously the question now is how good is our model at making these predictions, and one measure of that is \\(R^2\\) (pronounced R-squared). This relates to the \\(error\\) term that we saw in the original formula, i.e. \\(\\hat{Y} = b_{0} + b_{1}X + error\\), and as we said at the start it relates to the residuals. The residuals you might know are the difference between predicted values and observed values. For example, if we look at Participant 1 in our data we see they had a HeadSize of 50.8 and an IQ of 107. Those are our observed values of \\(X\\) and \\(Y\\) for that participant, but what if we predicted a value for that participant instead (\\(\\hat{Y}\\)). Using the above formula we would see: \\[\\hat{Y} = 48.75 + 1.194 \\times 50.8 = 109.4052\\] So we would predict for Participant 1, with a HeadSize of 50.8, to have an IQ of 109.4052. And if we compare that to the actually observed value for that participant (\\(Y = 107\\)) we see a difference of 2.4052 IQ points (i.e. \\(\\hat{Y} - Y = 2.4052\\)). And that difference is our residuals for that person - the left over part between the observed and the predicted. If we were then to check that for all the observed participants we would get an overall value that summarises how big or small our residuals are, and the smaller the residuals are, the better the model is at predicting values of \\(Y\\) from \\(X\\), because there is little difference between our predicted values and our observed values. And that is what our \\(R^2\\) tells us. It is a measure of how good our model is at prediction, and a measure of how small our residuals are. \\(R^2\\) can take values between 0 and 1 and simply put, the closer \\(R^2\\) is to 1, the smaller our residuals and the better our model is at prediction. Now there is a full approach to calculating the \\(R^2\\) which we will show you in a little bit, but when you are dealing with simple linear regression as we are here, there is a quick approach that gives the same answer, and it is: \\[R^2 = r_{(x,y)} \\times r_{(x,y)}\\] where \\(r_{(x,y)}\\) is the correlation between the two variables, in this case Headsize (\\(x\\)) and IQ (\\(y\\)). And if we look above we see we already know the correlation between HeadSize and IQ, as we calculated it in the correlation chapter. The correlation between HeadSize and IQ is \\(r_{(HeadSize, IQ)} = 0.967\\). And if we put that into our formula for \\(R^2\\) we see: \\[R^2 = 0.967 \\times 0.967 = 0.935089\\] Meaning that we have an \\(R^2\\) of \\(R^2 = 0.935\\), to three decimal places. And given that that is quite close to an \\(R^2\\) of 1, it would suggest that our model is very good at predicting IQ from HeadSize (Y from X), that our residuals are small, and that there is a strong positive relationship between the two variables. 7.5 The Write-up Ok great, so we have now got a lot of information on our model including the slope, the intercept, and the \\(R^2\\). The one thing we don’t actually know is where our model is significant. We will go into what that means and how to determine that in a later part, but in the meantime we will just tell you that the model is significant, (F(1, 8) = 115.07, p &lt; .001). So now we have everything we need for the write up: The model is significant - F(1, 8) = 115.07, p &lt; .001 The \\(R^2\\) = 0.935 is very high The only other element we need is the coefficent - the relationship between the two variables - which can take two versions; unstandardised and standardised. But, we actually do already know these. We just called them something else: what we call the slope, above, is the unstandardised coefficient and is written as \\(b\\) = 1.194 what we called the correlation between the variables is actually the standardised coefficient and is written as \\(\\beta\\) = 0.967 And it is totally acceptable to write-up either the standardised coefficient (\\(\\beta\\)) or the unstandardised coefficient (\\(b\\)). The main thing is to make sure you use the right symbol with the right value and don’t mix them up as that will mislead a reader. Now if we take all the information we have, including some information from the correlation chapter, we can put it into a blurb as such: A team of researchers were interested in the relationship between Head Size and IQ, and specifically whether they could predict IQ from Head Size. The researchers set out the hypothesis that a linear model based on head size, as measured in cms, will significantly predict IQ scores as measured on a standard IQ test. Descriptive analysis suggested a strong positive relationship between Head Size (M = 42.67, SD = 12.92) and IQ (M = 99.7, SD = 15.95), (r(8) = 0.967). On analysis, a linear regression model revealed that head size (measured in cm) significantly predicted participant scores on an IQ test (\\(\\beta\\) = 0.967, F(1, 8) = 115.07, p &lt; .001, \\(R^2\\) = 0.935), with head size explaining 93.5% of the variance in IQ scores. As such the alternative hypothesis was accepted suggesting that… But I guess it might look a bit odd as we give the standardised coeffiencient twice, just in different ways, so we could write it with the unstandardised coefficient as: A team of researchers were interested in the relationship between Head Size and IQ, and specifically whether they could predict IQ from Head Size. The researchers set out the hypothesis that a linear model based on head size, as measured in cms, will significantly predict IQ scores as measured on a standard IQ test. Descriptive analysis suggested a strong positive relationship between Head Size (M = 42.67, SD = 12.92) and IQ (M = 99.7, SD = 15.95), (r(8) = 0.967), with head size explaining 93.5% of the variance in IQ scores. On analysis, a linear regression model revealed that head size (measured in cm) significantly predicted participant scores on an IQ test (\\(b\\) = 1.194, F(1, 8) = 115.07, p &lt; .001, \\(R^2\\) = 0.935). As such the alternative hypothesis was accepted suggesting that… "],
["anova.html", "Chapter 8 ANOVA 8.1 The ANOVA table 8.2 Look-Up table", " Chapter 8 ANOVA This chapter is currently under development but will eventually show how to complete ANOVAs and ANOVA tables by hand. 8.1 The ANOVA table One-Way Between-Subjects Scenario: A research team are investigating the influence of mainstream media on life happiness. They record Life Happiness scores from 0 to 100 with higher scores meaning more happy with life from a large cohort of participants and split the participants into two groups; Group 1 (n = 72) and Group 2 (n = 81). The researchers want to check whether there is a difference between the two groups of participants in terms of Life Happiness. Calculate F from the below one-way ANOVA output table comparing mean Life Happiness scores across the two groups of participants and state whether there is a significant difference between the groups or not. The Sums of Squares (\\(SS\\)) have been given to you. Conditions \\(SS\\) \\(df\\) Mean Square F-value Between 44.56 Within 2126.3 Total 2170.86 In order to complete the above table we need to: determine the degrees of freedom (\\(df\\)) of the Between and the Within components use the \\(df\\) to calculate the Mean Squares (\\(MS\\)) use the Mean Squares to calculate the F-value Degrees of Freedom In an ANOVA there are two degrees of freedom. The first is the degrees of freedom related to the number of groups/conditions/models and is usually called \\(df_{Between}\\), \\(df_{Condition}\\), or \\(df_{Model}\\). They all refer to the same idea. Regardless of what you call it, this df is calculated as: \\[df_{between} = k - 1\\] Where \\(k\\) is the number of conditions or groups. The second degrees of freedom is related to the number of participants/observations/data-points and is usually called \\(df_{Within}\\) or \\(df_{Model}\\); again both referring to the same idea. This df is calculated as: \\[df_{within} = N - k\\] Where \\(N\\) is total number of participants in the whole experiment and \\(k\\) is again the number of conditions or groups. The \\(df_{within}\\) can also be phrased as the: \\[df_{within} = \\sum_1^k(n_{k}-1)\\] Which can be read as taking 1 away from the \\(n\\) in each group (\\(k\\)) and then summing those values together. For example, if you had a total of 20 participants in your experiment (\\(N = 20\\)) and they were split into 2 groups (\\(k = 2\\)) of 10 per group (\\(n = 10\\)), then you would do: \\[df_{within} = (10-1)+(10-1) = 18\\] But as that is the same as \\(N-k\\) (i.e. 20 - 2 = 18), and \\(N-k\\) is easier to think about, we will use that approach here. So, using those two formulas and the information given to us, first we see we have two groups. As such: \\[df_{between} = k - 1 = 2 - 1 = 1\\] Next we see that we have n = 72 in Group 1, and n = 81 in Group 2, meaning that we have a total number of participants of N = 72 + 81 = 153. And if we put that into the formula for the \\(df_{Within}\\) we see: \\[df_{Within} = N-k = 153 - 2 = 151\\] Meaning that we have \\(df_{Between}\\) = 1, and \\(df_{Within}\\) = 151, and we can fill those into our table as such: Conditions \\(SS\\) \\(df\\) Mean Square F-value Between 44.56 1 Within 2126.3 151 Total 2170.86 Mean Squares Great! Now the next step is to calculate the Mean Square value for both the Between (i.e. Groups) and Within (i.e. Residuals) components. The Mean Square is really an abbreviation of the Mean Sums of Squares and is the Sums of Squares of a component divided by the degrees of freedom. Or in other words: \\[MS = \\frac{SS}{df}\\] Looking at the table above we have all the information we need to calculate both the \\(MS_{Between}\\) and the \\(MS_{Within}\\). They would look like: \\[MS_{Between} = \\frac{SS_{Between}}{df_{Between}} = \\frac{44.56}{1} = 44.56\\] And \\[MS_{Within} = \\frac{SS_{Within}}{df_{Within}} = \\frac{2126.3}{151} = 14.081457\\] Meaning that we have \\(MS_{Between}\\) = 44.56 and \\(MS_{Within}\\) = 14.081457, which we can add in to our ANOVA table as such: Conditions \\(SS\\) \\(df\\) Mean Square F-value Between 44.56 1 44.56 Within 2126.3 151 14.081457 Total 2170.86 The F-value Awesome. Finally, we need to calculate our F-value which is the ratio between the \\(MS_{Between}\\) and \\(MS_{Within}\\), and can be written as: \\[F = \\frac{MS_{Between}}{MS_{Within}}\\] And so if we put our numbers in from above we see: \\[F = \\frac{MS_{Between}}{MS_{Within}} = \\frac{44.56}{14.081457} = 3.1644453\\] Giving us an F-value of F = 3.16 (to two decimal places), which we can now put into our table to complete our table as such: Conditions \\(SS\\) \\(df\\) Mean Square F-value Between 44.56 1 44.56 3.16 Within 2126.3 151 14.081457 Total 2170.86 Effect size There are a few effect sizes that people use for the ANOVA with one of the more common ones being what is called partial eta-squared with the symbol \\(\\eta_p^2\\) (as \\(\\eta\\) is the symbol for “eta”, and is pronounced “eat-ah”, and the \\(_p^2\\) indicating “partial” and “squared”). The formula for partial eta-squared is: \\[\\eta_{p^2} = \\frac{SS_{Between}}{SS_{Between}+SS_{Within}}\\] From above, we know that: \\(SS_{Between}\\) = 44.56 and \\(SS_{Within}\\) = 2126.3 And if we start to fill those into the formula we see: \\[\\eta_p^2 = \\frac{44.56}{44.56 + 2126.3} = \\frac{44.56}{2170.86} = 0.0205264\\] Meaning that this analysis has an effect size of \\(\\eta_p^2\\) = 0.02 (to two decimal places). The Write-UP The standard APA format for writing up an ANOVA is usually: F(\\(df_{Between}\\), \\(df_{Within}\\)) = F-value, p = p-value, \\(\\eta_p^2\\) = effectsize-value From above we know the two dfs and we know the F-value and the effect size, so what we need now is to determine the significance. Determining Significance If we were to look at a critical values look-up table for \\(df_{Between}\\) = 1, \\(df_{Within}\\) = 151, and \\(\\alpha = .05\\), we would see that the closest we have is for \\(df_{Between}\\) = 1, \\(df_{Within}\\) = 100, which has a critical value of \\(F_{crit}\\) = 3.94. Given that our F-value is smaller than our \\(F_{crit}\\) then we can say our result is not significant, and as such would be written up as F(1, 151) = 3.16, p &gt; .05, \\(\\eta_p^2\\) = 0.02. Remember: If you were writing this up as a report, and analysed the data in R, then you would see the p-value was actually p = 0.07747604, and would be written up as p = 0.077 8.2 Look-Up table Remembering that the \\(F_{crit}\\) value is the smallest F-value you need to find a significant effect, find the \\(F_{crit}\\) for your dfs, assuming \\(\\alpha = .05\\). If the \\(F\\)-value you calculated is equal to or larger than \\(F_{crit}\\) then your test is significant. In this table, to fit it on to the page, \\(df_{Between}\\) is written as df1, and \\(df_{Within}\\) is written as df2. dfs df1 = 1 df1 = 2 df1 = 3 df1 = 4 df1 = 5 df2 = 1 161.448 199.5 215.707 224.583 230.162 df2 = 2 18.513 19 19.164 19.247 19.296 df2 = 3 10.128 9.552 9.277 9.117 9.013 df2 = 4 7.709 6.944 6.591 6.388 6.256 df2 = 5 6.608 5.786 5.409 5.192 5.05 df2 = 10 4.965 4.103 3.708 3.478 3.326 df2 = 15 4.543 3.682 3.287 3.056 2.901 df2 = 20 4.351 3.493 3.098 2.866 2.711 df2 = 25 4.242 3.385 2.991 2.759 2.603 df2 = 30 4.171 3.316 2.922 2.69 2.534 df2 = 40 4.085 3.232 2.839 2.606 2.449 df2 = 50 4.034 3.183 2.79 2.557 2.4 df2 = 60 4.001 3.15 2.758 2.525 2.368 df2 = 70 3.978 3.128 2.736 2.503 2.346 df2 = 80 3.96 3.111 2.719 2.486 2.329 df2 = 90 3.947 3.098 2.706 2.473 2.316 df2 = 100 3.936 3.087 2.696 2.463 2.305 "],
["random-formulas.html", "Chapter 9 Random formulas 9.1 t-value to r-value", " Chapter 9 Random formulas These formulas will eventually move somewhere that makes sense but for now they need a home and this is it: 9.1 t-value to r-value Did you know that the conversion of a t-value to an r-value is: \\[r = \\sqrt\\frac{t^2}{t^2 + df}\\] And the conversion of an r-value to a t-value is: \\[t = \\sqrt\\frac{df * r^2}{1-r^2}\\] Where in both formulas, the \\(df\\) is the degrees of freedom of the respective analysis that has been carried out. For example, if you have carried out a correlation and have an r-value but want the t-value, use the df of the correlation to make the conversion. Likewise, if you have carried out a t-test and you want the r-value, use the df of the t-test to make the conversion. "],
["references.html", "Chapter 10 References", " Chapter 10 References "]
]
