# Correlation: Pearson

**The Pearson correlation** otherwise known as the Pearson Product-Moment correlation measures the relationshop between two variables when that relationship is monotonic and linear. To go a step further, in relating it measures the covariance between two variables, or the shared variance, and then standardises that measure between the values of -1 and 1 with -1 being the perfect negative relationship and 1 being the perfect positive relationship.

## The Worked Example

Let's say that this is our starting data:

```{r we, message=FALSE, warning=FALSE, echo=FALSE}

dat_o <- tibble(Participants = 1:10,
              HeadSize = c(50.8, 63.5, 45.7, 25.4, 29.2, 49.5, 38.1, 30.5, 35.6, 58.4),
              IQ = c(107, 121, 106, 72, 85, 105, 93, 88, 97, 123))

knitr::kable(dat_o, align = "c")
```

and the formula for the Pearson correlation is:

$$r_{(x,y)} = \frac{cov(x,y)}{\sigma_{x}\sigma_{y}}$$

Meaning that, assuming we consider HeadSize as variable $x$ and IQ as variable $y$, to determine the correlation between HeadSize and IQ ($r_{(HeadSize, IQ)}$) we first need to know the covariance between HeadSize and IQ, ($cov(HeadSize, IQ)$), as well as the standard deviation of HeadSize ($\sigma_{HeadSize}$) and the standard deviation of IQ ($\sigma_{IQ}$). That is what we are going to look at here. We will run it all through from start to finish, assuming all we know are the values. If you know certain aspects like the covariance and the standard deviations then you can skip to just running the correlation but it is good to see the analysis in its entirety.

**Covariance**

The formula for the covariance between two continuous variables is:

$$cov(x,y) = \frac{\sum_i^n(x_i - \overline{x})(y_i - \overline{y})}{n-1}$$

where the $\sum_i^n$ translates as sum (i.e. add up) the values of $(x_i - \overline{x})(y_i - \overline{y})$ for each pair of observations in your data. But to do that we first need to know the mean of x ($\overline{x}$) and the mean of y, ($\overline{y}$). Now we know that the mean of a value is the sum of all the values of that variable divided by the number of values - also written as: 

$$\overline{x} = \frac{\sum_i^n x_i}{n}$$. 

So the mean of x (i.e. HeadSize) is:

$$\overline{x} = \frac{`r dat_o %>% filter(Participants == 1) %>% pull(HeadSize)` + `r dat_o %>% filter(Participants == 2) %>% pull(HeadSize)` +  `r dat_o %>% filter(Participants == 3) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 4) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 5) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 6) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 7) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 8) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 9) %>% pull(HeadSize)`  + `r dat_o %>% filter(Participants == 10) %>% pull(HeadSize)`}{`r dim(dat_o)[1]`}$$

Which reduces to:

$$\overline{x} = \frac{`r dat_o %>% pull(HeadSize) %>% sum()`}{`r dim(dat_o)[1]`} = `r dat_o %>% pull(HeadSize) %>% sum()/dim(dat_o)[1]`$$

And if we repeat the same procedure for y (i.e. IQ), we get:

$$\overline{y} = \frac{`r dat_o %>% filter(Participants == 1) %>% pull(IQ)` + `r dat_o %>% filter(Participants == 2) %>% pull(IQ)` +  `r dat_o %>% filter(Participants == 3) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 4) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 5) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 6) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 7) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 8) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 9) %>% pull(IQ)`  + `r dat_o %>% filter(Participants == 10) %>% pull(IQ)`}{`r dim(dat_o)[1]`}$$

Which reduces to:

$$\overline{y} = \frac{`r dat_o %>% pull(IQ) %>% sum()`}{`r dim(dat_o)[1]`} = `r dat_o %>% pull(IQ) %>% sum()/dim(dat_o)[1]`$$

Ok great so we now have enough information to start filling in the covariance formula. First, however, before looking at it for all participants, let's look at this part, $(x_i - \overline{x})(y_i - \overline{y})$, for one participant just to make sure we understand what we are doing. And one thing to remember is that two sets of parentheses (i.e. brackets) side-by-side like that means to multiply them together. So, if we take Participant 1, then we see that:

* The mean of x is $\overline{x} = `r dat_o %>% pull(HeadSize) %>% sum()/dim(dat_o)[1]`$
* The mean of y is $\overline{y} = `r dat_o %>% pull(IQ) %>% sum()/dim(dat_o)[1]`$
* They have a HeadSize (x) of $x_1$ = `r dat_o %>% filter(Participants == 1) %>% pull(HeadSize)`
* They have an IQ (y) of $y_1$ = `r dat_o %>% filter(Participants == 1) %>% pull(IQ)`
* And note that we are now saying $x_1$ and $y_1$, and no longer $x_i$ and $y_i$, because we know we are specifically dealing with the 1^st^ Participant. If it was the 2^nd^ Participants we would write $x_2$ and $y_2$, and the 8^th^ would be $x_8$ and $y_8$, etc

Putting all that into the formula for Participant 1 (P~1~) would give us:

```{r cov_p1_vals, echo=FALSE}
xm <- dat_o %>% pull(HeadSize) %>% sum()/dim(dat_o)[1]
ym <- dat_o %>% pull(IQ) %>% sum()/dim(dat_o)[1]
x1 <- dat_o %>% filter(Participants == 1) %>% pull(HeadSize)
y1 <- dat_o %>% filter(Participants == 1) %>% pull(IQ)
x2 <- dat_o %>% filter(Participants == 2) %>% pull(HeadSize)
y2 <- dat_o %>% filter(Participants == 2) %>% pull(IQ)
x3 <- dat_o %>% filter(Participants == 3) %>% pull(HeadSize)
y3 <- dat_o %>% filter(Participants == 3) %>% pull(IQ)
x4 <- dat_o %>% filter(Participants == 4) %>% pull(HeadSize)
y4 <- dat_o %>% filter(Participants == 4) %>% pull(IQ)
x5 <- dat_o %>% filter(Participants == 5) %>% pull(HeadSize)
y5 <- dat_o %>% filter(Participants == 5) %>% pull(IQ)
x6 <- dat_o %>% filter(Participants == 6) %>% pull(HeadSize)
y6 <- dat_o %>% filter(Participants == 6) %>% pull(IQ)
x7 <- dat_o %>% filter(Participants == 7) %>% pull(HeadSize)
y7 <- dat_o %>% filter(Participants == 7) %>% pull(IQ)
x8 <- dat_o %>% filter(Participants == 8) %>% pull(HeadSize)
y8 <- dat_o %>% filter(Participants == 8) %>% pull(IQ)
x9 <- dat_o %>% filter(Participants == 9) %>% pull(HeadSize)
y9 <- dat_o %>% filter(Participants == 9) %>% pull(IQ)
x10 <- dat_o %>% filter(Participants == 10) %>% pull(HeadSize)
y10 <- dat_o %>% filter(Participants == 10) %>% pull(IQ)

  
```

P~1~ = $(x_1 - \overline{x})(y_1 - \overline{y}) = (`r x1` - `r xm`)\times(`r y1` - `r ym`) = `r x1 - xm` \times `r y1 - ym` = `r (x1 - xm)*(y1 - ym)`$

and we can do that for all participants which would look like:

* P~2~ = $(x_2 - \overline{x})(y_2 - \overline{y}) = (`r x2` - `r xm`)\times(`r y2` - `r ym`) = `r x2 - xm`\times`r y2 - ym` = `r (x2 - xm)*(y2 - ym)`$
* P~3~ = $(x_3 - \overline{x})(y_3 - \overline{y}) = (`r x3` - `r xm`)\times(`r y3` - `r ym`) = `r x3 - xm`\times`r y3 - ym` = `r (x3 - xm)*(y3 - ym)`$
* P~4~ = $(x_4 - \overline{x})(y_4 - \overline{y}) = (`r x4` - `r xm`)\times(`r y4` - `r ym`) = `r x4 - xm`\times`r y4 - ym` = `r (x4 - xm)*(y4 - ym)`$
* P~5~ = $(x_5 - \overline{x})(y_5 - \overline{y}) = (`r x5` - `r xm`)\times(`r y5` - `r ym`) = `r x5 - xm`\times`r y5 - ym` = `r (x5 - xm)*(y5 - ym)`$
* P~6~ = $(x_6 - \overline{x})(y_6 - \overline{y}) = (`r x6` - `r xm`)\times(`r y6` - `r ym`) = `r x6 - xm`\times`r y6 - ym` = `r (x6 - xm)*(y6 - ym)`$
* P~7~ = $(x_7 - \overline{x})(y_7 - \overline{y}) = (`r x7` - `r xm`)\times(`r y7` - `r ym`) = `r x7 - xm`\times`r y7 - ym` = `r (x7 - xm)*(y7 - ym)`$
* P~8~ = $(x_8 - \overline{x})(y_8 - \overline{y}) = (`r x8` - `r xm`)\times(`r y8` - `r ym`) = `r x8 - xm`\times`r y8 - ym` = `r (x8 - xm)*(y8 - ym)`$
* P~9~ = $(x_9 - \overline{x})(y_9 - \overline{y}) = (`r x9` - `r xm`)\times(`r y9` - `r ym`) = `r x9 - xm`\times`r y9 - ym` = `r (x9 - xm)*(y9 - ym)`$
* P~10~ = $(x_10 - \overline{x})(y_10 - \overline{y}) = (`r x10` - `r xm`)\times(`r y10` - `r ym`) = `r x10 - xm`\times`r y10 - ym` = `r (x10 - xm)*(y10 - ym)`$

And if we remember that the $\sum_i^n$ part of the formula means that the formula, for this example with 10 participants, is really

$$cov(x,y) = \frac{(x_1 - \overline{x})(y_1 - \overline{y})+(x_2 - \overline{x})(y_2 - \overline{y})+(x_3 - \overline{x})(y_3 - \overline{y})+(x_4 - \overline{x})(y_4 - \overline{y})+\\(x_5 - \overline{x})(y_5 - \overline{y})+(x_6 - \overline{x})(y_6 - \overline{y})+(x_7 - \overline{x})(y_7 - \overline{y})+(x_8 - \overline{x})(y_8 - \overline{y})+\\(x_9 - \overline{x})(y_9 - \overline{y})+(x_{10} - \overline{x})(y_{10} - \overline{y})}{n-1}$$

Then that becomes:

$$cov(x,y) = \frac{((`r x1` - `r xm`)\times(`r y1` - `r ym`)) + ((`r x2` - `r xm`)\times(`r y2` - `r ym`)) +\\ ((`r x3` - `r xm`)\times(`r y3` - `r ym`)) + ((`r x4` - `r xm`)\times(`r y4` - `r ym`)) +\\ ((`r x5` - `r xm`)\times(`r y5` - `r ym`)) + ((`r x6` - `r xm`)\times(`r y6` - `r ym`)) + \\((`r x7` - `r xm`)\times(`r y7` - `r ym`)) + ((`r x8` - `r xm`)\times(`r y8` - `r ym`)) + \\((`r x9` - `r xm`)\times(`r y9` - `r ym`)) + ((`r x10` - `r xm`)\times(`r y10` - `r ym`))}{n-1}$$

and, remembering the rules of BODMAS/PEMDAS, reduces to:

$$cov(x,y) = \frac{`r (x1 - xm)*(y1 - ym)` + `r (x2 - xm)*(y2 - ym)` + `r (x3 - xm)*(y3 - ym)` + `r (x4 - xm)*(y4 - ym)` + `r (x5 - xm)*(y5 - ym)`\\+ `r (x6 - xm)*(y6 - ym)` + `r (x7 - xm)*(y7 - ym)` + `r (x8 - xm)*(y8 - ym)` + `r (x9 - xm)*(y9 - ym)` + `r (x10 - xm)*(y10 - ym)`}{n-1}$$

And all these values match the value we calculated above, so we know that is correct. Then if we know n = `r dim(dat_o)[1]` as that is the number of participants, and so the bottom of the formula becomes n = `r dim(dat_o)[1]` - 1 = `r  dim(dat_o)[1] - 1`, we can reduce all of the above to:

$$cov(x,y) = \frac{`r ((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym))`}{`r dim(dat_o)[1] - 1`}$$

Leaving us with a covariance between HeadSize ($x$) and IQ ($y$) of:

$$cov(x,y) = `r (((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym)))/(dim(dat_o)[1] - 1)`$$

Great! Now if we look back at the formula for the Pearson correlation, we see:

$$r_{(x,y)} = \frac{cov(x,y)}{\sigma_{x}\sigma_{y}}$$

So the covariance of x and y, divided by the product (multiply them together) of the standard deviation of x $\sigma_{x}$ (pronounced sigma-x) and the standard deviation of y $\sigma_{y}$ (pronounced sigma-y). Remember that $\sigma$ can also be written as $S$ or $SD$ and is the symbol for the Standard Deviation and that $\sigma^2$ (pronounced sigma-squared) is the symbol for the variance. So in reality, the pearson correlation between Head Size and IQ is written as:

$$r_{(HeadSize,IQ)} = \frac{cov(HeadSize,IQ)}{\sigma_{HeadSize}\times\sigma_{IQ}}$$

And if we known from above that $cov(HeadSize, IQ)$ = `r round2((((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym)))/(dim(dat_o)[1] - 1),2)` (to two decimal places), now all we need is the standard deviation of Headsize ($\sigma_{HeadSize}$) and the standard deviation of IQ ($\sigma_{IQ}$).

We cover standard deviation calculations in the Descriptives chapter, but to briefly recap, the formula is:

$$\sigma = \sqrt\frac{\sum_i^n(x_{i} - \overline{x})^2}{n-1}$$

Which for HeadSize would look like:

$$\sigma_{HeadSize} = \sqrt\frac{(`r x1` - `r xm`)^2 + (`r x2` - `r xm`)^2 +(`r x3` - `r xm`)^2 +(`r x4` - `r xm`)^2 +\\(`r x5` - `r xm`)^2 +(`r x6` - `r xm`)^2 +(`r x7` - `r xm`)^2 +(`r x8` - `r xm`)^2 +\\(`r x9` - `r xm`)^2 +(`r x10` - `r xm`)^2}{`r dim(dat_o)[1]`-1}$$

Which reduces to:

$$\sigma_{HeadSize} = \sqrt\frac{`r (x1 - xm)^2` + `r (x2 - xm)^2` +`r (x3 - xm)^2` +`r (x4 - xm)^2` +\\`r (x5 - xm)^2` +`r (x6 - xm)^2` +`r (x7 - xm)^2` +`r (x8 - xm)^2` +\\`r (x9 - xm)^2` +`r (x10 - xm)^2` }{`r dim(dat_o)[1]-1`}$$

Giving us:

$$\sigma_{HeadSize} = \sqrt\frac{`r ((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2)`}{`r dim(dat_o)[1]-1`} = \sqrt{`r (((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2))/(dim(dat_o)[1]-1)`} = `r sqrt((((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2))/(dim(dat_o)[1]-1))`$$

And if we repeat that process for the standard deviation of IQ ($\sigma_{IQ}$) then it would be:

$$\sigma_{IQ} = \sqrt\frac{(`r y1` - `r ym`)^2 + (`r y2` - `r ym`)^2 +(`r y3` - `r ym`)^2 +(`r y4` - `r ym`)^2 +(`r y5` - `r ym`)^2 +\\(`r y6` - `r ym`)^2 +(`r y7` - `r ym`)^2 +(`r y8` - `r ym`)^2 +(`r y9` - `r ym`)^2 +(`r y10` - `r ym`)^2}{`r dim(dat_o)[1]`-1}$$

Which reduces to:

$$\sigma_{IQ} = \sqrt\frac{`r (y1 - ym)^2` + `r (y2 - ym)^2` +`r (y3 - ym)^2` +`r (y4 - ym)^2` +`r (y5 - ym)^2` +\\`r (y6 - ym)^2` +`r (y7 - ym)^2` +`r (y8 - ym)^2` +`r (y9 - ym)^2` +`r (y10 - ym)^2` }{`r dim(dat_o)[1]-1`}$$

Giving us:

$$\sigma_{IQ} = \sqrt\frac{`r ((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2)`}{`r dim(dat_o)[1]-1`} = \sqrt{`r (((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2))/(dim(dat_o)[1]-1)`} = `r sqrt((((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2))/(dim(dat_o)[1]-1))`$$

And so if we know the following,

* $cov(x,y)$ = $cov(HeadSpace, IQ)$ = `r round2((((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym)))/(dim(dat_o)[1] - 1),2)`
* $\sigma_{x}$ = $\sigma_{HeadSpace}$ = `r round2(sqrt((((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2))/(dim(dat_o)[1]-1)),2)`
* $\sigma_{y}$ = $\sigma_{IQ}$ = `r round2(sqrt((((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2))/(dim(dat_o)[1]-1)),2)`

Then our correlation value is:

\begin{align*}
r_{(HeadSize,IQ)} &= \frac{cov(HeadSize,IQ)}{\sigma_{HeadSize}\times\sigma_{IQ}} \\ \\ &= \frac{`r round2((((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym)))/(dim(dat_o)[1] - 1),2)`}{`r round2(sqrt((((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2))/(dim(dat_o)[1]-1)),2)` \times `r round2(sqrt((((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2))/(dim(dat_o)[1]-1)),2)`} \\ \\ &= \frac{`r round2((((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym)))/(dim(dat_o)[1] - 1),2)`}{`r (round2(sqrt((((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2))/(dim(dat_o)[1]-1)),2)) *(round2(sqrt((((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2))/(dim(dat_o)[1]-1)),2))`} \\ \\ &= `r (round2((((x1 - xm)*(y1 - ym)) + ((x2 - xm)*(y2 - ym)) + ((x3 - xm)*(y3 - ym)) + ((x4 - xm)*(y4 - ym)) + ((x5 - xm)*(y5 - ym)) + ((x6 - xm)*(y6 - ym)) + ((x7 - xm)*(y7 - ym)) + ((x8 - xm)*(y8 - ym)) + ((x9 - xm)*(y9 - ym)) + ((x10 - xm)*(y10 - ym)))/(dim(dat_o)[1] - 1),2))/((round2(sqrt((((x1 - xm)^2) + ((x2 - xm)^2)+ ((x3 - xm)^2)+ ((x4 - xm)^2)+ ((x5 - xm)^2)+ ((x6 - xm)^2)+ ((x7 - xm)^2)+ ((x8 - xm)^2)+ ((x9 - xm)^2)+ ((x10 - xm)^2))/(dim(dat_o)[1]-1)),2)) *(round2(sqrt((((y1 - ym)^2) + ((y2 - ym)^2)+ ((y3 - ym)^2)+ ((y4 - ym)^2)+ ((y5 - ym)^2)+ ((y6 - ym)^2)+ ((y7 - ym)^2)+ ((y8 - ym)^2)+ ((y9 - ym)^2)+ ((y10 - ym)^2))/(dim(dat_o)[1]-1)),2)))`
\end{align*}

```{r cor_vals, echo = FALSE}
corval <- cor.test(dat_o$HeadSize, dat_o$IQ, method = "pearson") %>% pluck("estimate") %>% round2(3)
df <- cor.test(dat_o$HeadSize, dat_o$IQ, method = "pearson") %>% pluck("parameter")
realt <- cor.test(dat_o$HeadSize, dat_o$IQ, method = "pearson") %>% pluck("statistic")
realp <- pt(realt, df, lower.tail = FALSE)*2

strongcor <- .45
medcor <- .25
weakcor <- .1
rcrit <- sqrt(qt(1-.975, df, lower.tail = FALSE)^2/(qt(1-.975, df, lower.tail = FALSE)^2 + df)) %>% round2(3)


```


Meaning that we would have a correlation value of, to three decimal places, $r$ = `r corval`, which, following Cohen's (1988) proposed cut-offs shown below, would be considered a strong positive relationship.

|Relationship Description (Cohen, 1988)| closest r-value|
|---:|:---:|
|Strong positive relationship| r = .5|
|Medium positive relationship| r = .3|
|Weak positive relationship| r = .1|
|No relationship| r = 0| 
|Weak negative relationship| r = -.1|
|Medium negative relationship| r = -.3|
|Strong negative relationship| r = -.5|

**Degrees of Freedom**

Great! Now, finally, to determine significance we first need the degrees of freedom for the test. The degrees of freedom formula is:

$$df = N - 2$$

And we already know that $N= `r dim(dat_o)[1]`$ and putting them into the equation we get:

$$df = `r dim(dat_o)[1]` - 2$$

Which reduces to:

$$df = `r dim(dat_o)[1] - 2`$$


Meaning that we find a $df = `r dim(dat_o)[1] - 2`$

**Determining Significance**

If we were to look at a critical values look-up table for $df = `r df`$ and $\alpha = .05$, we would see that the critical value is $r_{crit} = `r rcrit`$. Given that our r-value, ignoring polarity and just looking at the absolute value, so $r = `r corval`$, is `r ifelse(abs(corval) >= rcrit, "equal to or larger than", "smaller than")` our $r_{crit}$ then we can say our result is `r ifelse(abs(corval) >= rcrit, "significant", "not significant")`, and as such would be written up as r(`r df`) = `r corval`, `r ifelse(abs(corval) >= rcrit, "p < .05", "p > .05")`. 

**Remember:** If you were writing this up as a report, and analysed the data in R, then you would see the p-value was actually p = `r format(realp, scientific = FALSE)`, and would be written up as `r ifelse(realp < .001, "p < .001", paste("p = ", realp))`

## Look-Up table

Remembering that the $r_{crit}$ value is the smallest r-value you need to find a significant effect, find the $r_{crit}$ value for your df, assuming $\alpha = .05$. If the $r$-value you calculated is equal to or larger than $r_{crit}$ value then your test is significant.

|df | $\alpha = .05$|
|:-:|:-------------:|
|1  |`r sqrt(qt(1-.975, 1, lower.tail = FALSE)^2/(qt(1-.975, 1, lower.tail = FALSE)^2 + 1)) %>% round2(3)`|
|2  |`r sqrt(qt(1-.975, 2, lower.tail = FALSE)^2/(qt(1-.975, 2, lower.tail = FALSE)^2 + 2)) %>% round2(3)`|
|3  |`r sqrt(qt(1-.975, 3, lower.tail = FALSE)^2/(qt(1-.975, 3, lower.tail = FALSE)^2 + 3)) %>% round2(3)`|
|4  |`r sqrt(qt(1-.975, 4, lower.tail = FALSE)^2/(qt(1-.975, 4, lower.tail = FALSE)^2 + 4)) %>% round2(3)`|
|5  |`r sqrt(qt(1-.975, 5, lower.tail = FALSE)^2/(qt(1-.975, 5, lower.tail = FALSE)^2 + 5)) %>% round2(3)`|
|6  |`r sqrt(qt(1-.975, 6, lower.tail = FALSE)^2/(qt(1-.975, 6, lower.tail = FALSE)^2 + 6)) %>% round2(3)`|
|7  |`r sqrt(qt(1-.975, 7, lower.tail = FALSE)^2/(qt(1-.975, 7, lower.tail = FALSE)^2 + 7)) %>% round2(3)`|
|8  |`r sqrt(qt(1-.975, 8, lower.tail = FALSE)^2/(qt(1-.975, 8, lower.tail = FALSE)^2 + 8)) %>% round2(3)`|
|9  |`r sqrt(qt(1-.975, 9, lower.tail = FALSE)^2/(qt(1-.975, 9, lower.tail = FALSE)^2 + 9)) %>% round2(3)`|
|10 |`r sqrt(qt(1-.975, 10, lower.tail = FALSE)^2/(qt(1-.975, 10, lower.tail = FALSE)^2 + 10)) %>% round2(3)`|
|15 |`r sqrt(qt(1-.975, 15, lower.tail = FALSE)^2/(qt(1-.975, 15, lower.tail = FALSE)^2 + 15)) %>% round2(3)`|
|20 |`r sqrt(qt(1-.975, 20, lower.tail = FALSE)^2/(qt(1-.975, 20, lower.tail = FALSE)^2 + 20)) %>% round2(3)`|
|30 |`r sqrt(qt(1-.975, 30, lower.tail = FALSE)^2/(qt(1-.975, 30, lower.tail = FALSE)^2 + 30)) %>% round2(3)`|
|40 |`r sqrt(qt(1-.975, 40, lower.tail = FALSE)^2/(qt(1-.975, 40, lower.tail = FALSE)^2 + 40)) %>% round2(3)`|
|50 |`r sqrt(qt(1-.975, 50, lower.tail = FALSE)^2/(qt(1-.975, 50, lower.tail = FALSE)^2 + 50)) %>% round2(3)`|
|60 |`r sqrt(qt(1-.975, 60, lower.tail = FALSE)^2/(qt(1-.975, 60, lower.tail = FALSE)^2 + 60)) %>% round2(3)`|
|70 |`r sqrt(qt(1-.975, 70, lower.tail = FALSE)^2/(qt(1-.975, 70, lower.tail = FALSE)^2 + 70)) %>% round2(3)`|
|80 |`r sqrt(qt(1-.975, 80, lower.tail = FALSE)^2/(qt(1-.975, 80, lower.tail = FALSE)^2 + 80)) %>% round2(3)`|
|90 |`r sqrt(qt(1-.975, 90, lower.tail = FALSE)^2/(qt(1-.975, 90, lower.tail = FALSE)^2 + 90)) %>% round2(3)`|
|100|`r sqrt(qt(1-.975, 100, lower.tail = FALSE)^2/(qt(1-.975, 100, lower.tail = FALSE)^2 + 100)) %>% round2(3)`|
